# 多模态活动预测系统 - 主要代码文件
===============================================
本文件包含了多模态活动预测系统的主要代码文件：
1. 主训练脚本 (train_new.py)
2. 主模型 (WXM.py)
3. DDN归一化模块 (DDN.py)
4. 数据处理模块 (data_processor.py)
5. 静态预测实验 (static_exp.py)
6. 动态预测实验 (dynamic_exp.py)
7. RevIN可逆归一化层 (RevIN.py)
8. 特征分布标准化层 (fds.py)
9. 工具函数 (tools.py)
10. 配置文件 (config.yaml)
11.原始数据预处理工具
    11.1 活动属性表生成工具 (活动属性表生成工具.py)
    11.2 活动订单数据分割工具 (活动订单数据分割工具.py)
===============================================
1. 主训练脚本 - train_new.py
===============================================
功能：主训练入口，配置加载，参数转换，训练流程控制


#!/usr/bin/env python3
"""
新的训练脚本
使用配置文件管理参数，整合优化后的模块
"""
import os
import yaml
import argparse
import pandas as pd
import json
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split

from utils.data_processor import DataProcessor, UnifiedDataset
from utils.static_exp import StaticExp
from utils.dynamic_exp import DynamicExp
from models.WXM import WXM
import warnings

# 忽略警告
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['CURL_CA_BUNDLE'] = ''


def load_config(config_path: str) -> dict:
    """加载配置文件"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def save_config(config: dict, save_path: str):
    """保存配置到JSON格式（用于兼容现有代码）"""
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    with open(save_path, 'w') as f:
        json.dump(config, f, indent=2)


def convert_config_to_args(config: dict) -> argparse.Namespace:
    """将配置字典转换为argparse.Namespace对象（用于兼容现有代码）"""
    args = argparse.Namespace()
    
    # 基础配置
    args.random_seed = config.get('random_seed', 2024)
    args.gpu_num = config.get('gpu_num', 0)
    args.log_dir = config.get('log_dir', 'log')
    
    # 数据路径配置
    data_config = config.get('data', {})
    args.oringinal_data_path = data_config.get('original_data_path', 'data/activity_order_independent_id')
    args.historical_activities_path = data_config.get('historical_activities_path', 'data')
    args.activities_attributes_path = data_config.get('activities_attributes_path', 'data')
    args.dataset_df_path = data_config.get('dataset_df_path', 'data/dataset_df.csv')
    
    # 预测目标配置
    prediction_config = config.get('prediction', {})
    args.freq = prediction_config.get('freq', 'd')
    args.target = prediction_config.get('target', 'uc')
    args.static_target = prediction_config.get('static_target', 'user_sum')
    
    # 模型配置
    model_config = config.get('model', {})
    args.model_type = model_config.get('model_type', 'WXM')
    args.embedding_dim = model_config.get('embedding_dim', 32)
    args.d_model = model_config.get('d_model', 128)
    args.d_s = model_config.get('d_s', 64)
    args.d_n = model_config.get('d_n', 64)
    args.num_attn_heads = model_config.get('num_attn_heads', 1)
    args.num_hidden_layers = model_config.get('num_hidden_layers', 2)
    args.dropout = model_config.get('dropout', 0.4)
    args.input_dim = 1
    args.use_encoder_mask = model_config.get('use_encoder_mask', 1)
    args.autoregressive = model_config.get('autoregressive', 0)
    
    # 特征配置
    features_config = config.get('features', {})
    args.use_img = features_config.get('use_img', False)
    args.use_text_attributes = features_config.get('use_text_attributes', True)
    args.use_label_attributes = features_config.get('use_label_attributes', True)
    args.use_numeric_attributes = features_config.get('use_numeric_attributes', True)
    args.use_temporal_features = features_config.get('use_temporal_features', True)
    args.use_current_seq = features_config.get('use_current_seq', True)
    args.use_similar = features_config.get('use_similar', False)
    
    # 训练配置
    training_config = config.get('training', {})
    args.batch_size = training_config.get('batch_size', 8)
    args.use_lr_scheduler = training_config.get('use_lr_scheduler', True)
    args.loss = training_config.get('loss', 'mae')

    # 静态预测训练参数
    static_training_config = config.get('static_training', {})
    args.static_epochs = static_training_config.get('static_epochs', 10)
    args.static_learning_rate = static_training_config.get('static_learning_rate', 0.1)
    args.static_output_type = static_training_config.get('static_output_type', 'quantile')
    args.num_gated_blocks = static_training_config.get('num_gated_blocks', 2)

    # 动态预测训练参数
    dynamic_training_config = config.get('dynamic_training', {})
    args.dynamic_epochs = dynamic_training_config.get('dynamic_epochs', 100)
    args.dynamic_learning_rate = dynamic_training_config.get('dynamic_learning_rate', 0.1)
    
    # 序列配置
    sequence_config = config.get('sequence', {})
    args.current_seq_len = sequence_config.get('current_seq_len', 14)
    args.predict_seq_len = sequence_config.get('predict_seq_len', 7)
    args.total_seq_len = sequence_config.get('total_seq_len', 30)
    args.sequence_process_type = sequence_config.get('sequence_process_type', 'LTTB')
    args.apply_diff = sequence_config.get('apply_diff', True)
    args.apply_smoothing = sequence_config.get('apply_smoothing', True)
    args.series_scale = sequence_config.get('series_scale', False)
    args.feature_scale = sequence_config.get('feature_scale', True)
    
    # DDN配置
    ddn_config = config.get('ddn', {})
    args.use_ddn_normalization = ddn_config.get('use_ddn_normalization', True)
    args.j = ddn_config.get('j', 0)
    args.learnable = ddn_config.get('learnable', False)
    args.wavelet = ddn_config.get('wavelet', 'coif3')
    args.dr = ddn_config.get('dr', 0.01)
    args.pre_epoch = ddn_config.get('pre_epoch', 5)
    args.twice_epoch = ddn_config.get('twice_epoch', 1)
    args.use_norm = ddn_config.get('use_norm', 'sliding')
    args.kernel_len = ddn_config.get('kernel_len', 7)
    args.hkernel_len = ddn_config.get('hkernel_len', 5)
    args.station_lr = ddn_config.get('station_lr', 0.0001)
    args.station_type = ddn_config.get('station_type', 'adaptive')
    args.pd_ff = ddn_config.get('pd_ff', 1024)
    args.pd_model = ddn_config.get('pd_model', 512)
    args.pe_layers = ddn_config.get('pe_layers', 2)
    
    # 其他参数
    args.process_data = False  # 默认不重新处理数据
    args.task_type = 'static'  # 默认静态预测，可通过命令行参数覆盖
    
    return args


def run_training(config_path: str, task_type: str = 'static', process_data: bool = False):
    """运行训练"""
    print(f"使用配置文件: {config_path}")
    print(f"任务类型: {task_type}")
    
    # 加载配置
    config = load_config(config_path)
    args = convert_config_to_args(config)
    args.task_type = task_type
    args.process_data = process_data
    
    print("配置参数:")
    print(f"  任务类型: {args.task_type}")
    print(f"  模型类型: {args.model_type}")
    print(f"  批次大小: {args.batch_size}")
    print(f"  学习率: {args.static_learning_rate if task_type == 'static' else args.dynamic_learning_rate}")
    
    # 设置随机种子
    StaticExp.set_seed(args.random_seed)
    
    # 数据处理
    data_processor = DataProcessor(config)
    
    # 检查是否需要重新处理数据
    need_reprocess = args.process_data
    
    # 检查数据集是否存在
    if os.path.exists(args.dataset_df_path):
        # 如果数据集存在，检查是否包含序列数据
        dataset_df = pd.read_csv(args.dataset_df_path)
    else:
        print(f"数据集文件不存在: {args.dataset_df_path}")
        need_reprocess = True
    
    # 如果需要重新处理数据
    if need_reprocess:
        print("开始处理原始数据...")
        data_processor.process_activities(
            args.oringinal_data_path, 
            args.historical_activities_path, 
            args.freq
        )
        
        # 处理完成后，创建数据集
        print("创建数据集...")
        dataset_df = data_processor.create_dataset(
            task_type=task_type,
            sequence_process_type=args.sequence_process_type,
            current_seq_len=args.current_seq_len,
            predict_seq_len=args.predict_seq_len,
            total_seq_len=args.total_seq_len,
            oringinal_data_path=args.oringinal_data_path,
            historical_activities_path=args.historical_activities_path,
            target=args.target,
            freq=args.freq,
            apply_diff=args.apply_diff,
            apply_smoothing=args.apply_smoothing,
            activities_attributes_path=args.activities_attributes_path
        )
        
        # 保存数据集
        os.makedirs(os.path.dirname(args.dataset_df_path), exist_ok=True)
        dataset_df.to_csv(args.dataset_df_path, index=False)
        print(f"数据集已保存到: {args.dataset_df_path}")
    
    # 再次检查数据集是否存在
    if not os.path.exists(args.dataset_df_path):
        print(f"数据集文件不存在: {args.dataset_df_path}")
        print("数据处理失败，请检查原始数据路径")
        return
    
    # 加载数据集
    print("加载数据集...")
    dataset_df = pd.read_csv(args.dataset_df_path)
    print(f"数据集大小: {len(dataset_df)} 行")
    
    # 打印数据集详细信息
    print("\n" + "="*50)
    print("数据集详细信息")
    print("="*50)
    print(f"数据集文件: {args.dataset_df_path}")
    print(f"总样本数: {len(dataset_df)}")
    print(f"特征数量: {len(dataset_df.columns)}")
    print(f"任务类型: {task_type}")
    
    # 打印目标变量信息
    if task_type == 'static':
        target_col = args.static_target
        print(f"目标变量: {target_col}")
        if target_col in dataset_df.columns:
            target_stats = dataset_df[target_col].describe()
            print(f"目标变量统计:")
            print(f"  均值: {target_stats['mean']:.2f}")
            print(f"  标准差: {target_stats['std']:.2f}")
            print(f"  最小值: {target_stats['min']:.2f}")
            print(f"  最大值: {target_stats['max']:.2f}")
    else:
        print(f"目标变量: {args.target} (时间序列)")
        print(f"序列长度: {args.current_seq_len} + {args.predict_seq_len} = {args.current_seq_len + args.predict_seq_len}")
        
        # 检查序列数据列
        sequence_columns = [col for col in dataset_df.columns if 'seq_' in col or 'sequence' in col]
        if sequence_columns:
            print(f"序列数据列: {len(sequence_columns)} 个")
            for i, col in enumerate(sequence_columns[:5]):  # 只显示前5个
                print(f"  {i+1}. {col}")
            if len(sequence_columns) > 5:
                print(f"  ... 还有 {len(sequence_columns) - 5} 个序列列")
    
    # 打印特征信息
    print(f"\n特征列:")
    for i, col in enumerate(dataset_df.columns[:10]):  # 只显示前10列
        print(f"  {i+1:2d}. {col}")
    if len(dataset_df.columns) > 10:
        print(f"  ... 还有 {len(dataset_df.columns) - 10} 个特征")
    
    # 检查缺失值
    missing_values = dataset_df.isnull().sum()
    if missing_values.sum() > 0:
        print(f"\n缺失值情况:")
        for col, missing in missing_values[missing_values > 0].items():
            print(f"  {col}: {missing} 个缺失值")
    else:
        print(f"\n缺失值: 无")
    
    print("="*50 + "\n")
    
    # 分割数据集
    train_dataset_df, test_dataset_df = train_test_split(
        dataset_df, test_size=0.2, shuffle=False, random_state=args.random_seed
    )
    
    # 创建数据加载器
    sequence_len = args.current_seq_len + args.predict_seq_len if task_type == 'dynamic' else args.total_seq_len
    
    train_dataset = UnifiedDataset(
        train_dataset_df, config, task_type, 'train'
    )
    test_dataset = UnifiedDataset(
        test_dataset_df, config, task_type, 'test'
    )
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # 创建模型
    print("创建模型...")
    model = WXM(args)
    
    # 选择实验类型
    if task_type == 'static':
        exp = StaticExp(args)
        epochs = args.static_epochs
    else:
        exp = DynamicExp(args)
        epochs = args.dynamic_epochs
    
    # 运行训练
    print(f"开始训练，共 {epochs} 个epoch...")
    predictions, true_values = exp.run(train_loader, test_loader)
    
    print("训练完成！")
    return predictions, true_values


def main():
    """主函数"""
    parser = argparse.ArgumentParser(description='多模态活动预测系统')
    parser.add_argument('--config', type=str, default='configs/config.yaml', 
                       help='配置文件路径')
    parser.add_argument('--task_type', type=str, default='dynamic', 
                       choices=['static', 'dynamic'], help='任务类型')
    parser.add_argument('--process_data', action='store_true', 
                       help='是否重新处理数据，通常修改task_type、current_seq_len、predict_seq_len、freq时需要重新处理数据')
    
    args = parser.parse_args()
    
    # 检查配置文件是否存在
    if not os.path.exists(args.config):
        print(f"配置文件不存在: {args.config}")
        print("请确保配置文件存在或使用 --config 参数指定正确的路径")
        return
    
    # 检查数据集是否存在，如果不存在且是动态预测，提示用户
    config = load_config(args.config)
    args_config = convert_config_to_args(config)
    dataset_path = args_config.dataset_df_path
    
    if not os.path.exists(dataset_path) and args.task_type == 'dynamic':
        print(f"注意: 数据集文件不存在 ({dataset_path})")
        print("动态预测需要序列数据，将自动重新处理数据...")
        args.process_data = True
    
    # 运行训练
    run_training(args.config, args.task_type, args.process_data)


if __name__ == '__main__':
    main() 


===============================================
2. 主模型 - models/WXM.py
===============================================
功能：WXM多模态融合模型，支持文本、数值、标签、时间特征融合


import math
import os
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel
from torchvision.models import resnet50, ResNet50_Weights

from layers.RevIN import RevIN


@dataclass
class ModelConfig:
    """模型配置类，统一管理所有模型参数"""
    
    # 基础配置
    embedding_dim: int = 128
    hidden_dim: int = 256
    d_model: int = 128
    d_s: int = 64
    d_n: int = 64
    
    # 注意力配置
    num_heads: int = 8
    num_layers: int = 2
    
    # 序列配置
    current_seq_len: int = 14
    predict_seq_len: int = 7
    total_seq_len: int = 30
    input_dim: int = 1
    
    # 功能开关
    use_img: bool = False
    use_text_attributes: bool = True
    use_label_attributes: bool = True
    use_numeric_attributes: bool = True
    use_temporal_features: bool = True
    use_current_seq: bool = True
    use_similar: bool = False
    use_encoder_mask: bool = False
    autoregressive: bool = False
    
    # 训练配置
    dropout: float = 0.1
    gpu_num: int = 0
    task_type: str = 'static'  # 'static' or 'dynamic'


class TextEmbedder(nn.Module):
    """文本嵌入模块，使用BERT进行文本特征提取"""
    
    def __init__(self, embedding_dim: int, gpu_num: int, dropout: float = 0.1):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.gpu_num = gpu_num
        
        # BERT模型初始化（使用本地模型文件）
        bert_model_path = os.path.join(os.path.dirname(__file__), '..', 'bert-base-uncased')
        self.tokenizer = BertTokenizer.from_pretrained(bert_model_path)
        self.bert_model = BertModel.from_pretrained(bert_model_path)
        
        # 投影层
        self.fc = nn.Linear(768, embedding_dim)
        self.dropout = nn.Dropout(dropout)
        
        # 设备配置
        self.device = torch.device(f'cuda:{gpu_num}' if torch.cuda.is_available() else 'cpu')
    
    def forward(self, activity_name: List[str], activity_title: List[str], 
                product_names: List[str]) -> torch.Tensor:
        """前向传播
        
        Args:
            activity_name: 活动名称列表
            activity_title: 活动标题列表  
            product_names: 产品名称列表
            
        Returns:
            torch.Tensor: 文本嵌入特征 [batch_size, embedding_dim]
        """
        # 文本拼接
        textual_descriptions = [
            f"{name} {title} {product}" 
            for name, title, product in zip(activity_name, activity_title, product_names)
        ]
        
        # BERT编码
        inputs = self.tokenizer(
            textual_descriptions, 
            padding=True, 
            truncation=True, 
            max_length=512, 
            return_tensors="pt"
        )
        
        # 设备转移
        input_ids = inputs['input_ids'].to(self.device)
        attention_mask = inputs['attention_mask'].to(self.device)
        
        # BERT前向传播
        with torch.no_grad():  # 冻结BERT参数
            outputs = self.bert_model(input_ids, attention_mask=attention_mask)
        
        # 特征提取和投影
        pooled_output = outputs.pooler_output
        embeddings = self.dropout(self.fc(pooled_output))
        
        return embeddings


class ImageEmbedder(nn.Module):
    """图像嵌入模块，使用ResNet50提取图像特征"""
    
    def __init__(self):
        super().__init__()
        # 图像特征提取
        weights = ResNet50_Weights.IMAGENET1K_V1
        resnet = resnet50(weights=weights)
        modules = list(resnet.children())[:-2]
        self.resnet = nn.Sequential(*modules)
        
        # 冻结ResNet参数
        for p in self.resnet.parameters():
            p.requires_grad = False
        
    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """前向传播
        
        Args:
            images: 输入图像 [batch_size, channels, height, width]
            
        Returns:
            torch.Tensor: 图像特征 [batch_size, 2048, h/32, w/32]
        """
        img_embeddings = self.resnet(images)
        size = img_embeddings.size()
        out = img_embeddings.view(*size[:2], -1)
        return out.view(*size).contiguous()


class DummyEmbedder(nn.Module):
    """时间特征嵌入模块，处理日、周、月、年等时间特征"""
    
    def __init__(self, embedding_dim: int, dropout: float = 0.1):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # 各时间维度嵌入层
        self.time_embeddings = nn.ModuleDict({
            'day': nn.Linear(1, embedding_dim),
            'week': nn.Linear(1, embedding_dim), 
            'month': nn.Linear(1, embedding_dim),
            'year': nn.Linear(1, embedding_dim)
        })
        
        # 特征融合层
        self.fusion_layer = nn.Linear(embedding_dim * 4, embedding_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, temporal_features: torch.Tensor) -> torch.Tensor:
        """前向传播
        
        Args:
            temporal_features: 时间特征 [batch_size, 4] (day, week, month, year)
            
        Returns:
            torch.Tensor: 时间嵌入特征 [batch_size, embedding_dim]
        """
        temporal_features = temporal_features.float()
        
        # 特征分离
        day_feat = temporal_features[:, 0:1]    # [batch_size, 1]
        week_feat = temporal_features[:, 1:2]   # [batch_size, 1]
        month_feat = temporal_features[:, 2:3]  # [batch_size, 1]
        year_feat = temporal_features[:, 3:4]   # [batch_size, 1]
        
        # 各维度嵌入
        embeddings = {
            'day': self.time_embeddings['day'](day_feat),
            'week': self.time_embeddings['week'](week_feat),
            'month': self.time_embeddings['month'](month_feat),
            'year': self.time_embeddings['year'](year_feat)
        }
        
        # 特征融合
        concatenated = torch.cat(list(embeddings.values()), dim=1)
        fused_embeddings = self.dropout(self.fusion_layer(concatenated))
        
        return fused_embeddings


class CrossModalAttention(nn.Module):
    """跨模态注意力模块，用于融合不同模态的信息"""
    
    def __init__(self, embed_dim: int, num_heads: int):
        super().__init__()
        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.layer_norm = nn.LayerNorm(embed_dim)
        
    def forward(self, query: torch.Tensor, key: torch.Tensor, 
                value: torch.Tensor) -> torch.Tensor:
        """前向传播
        
        Args:
            query: 查询张量 [N, embed_dim]
            key: 键张量 [N, embed_dim]
            value: 值张量 [N, embed_dim]
            
        Returns:
            torch.Tensor: 注意力输出 [N, embed_dim]
        """
        attn_output, _ = self.multihead_attn(
            query.unsqueeze(0),  # [1, N, D]
            key.unsqueeze(0),    # [1, N, D]
            value.unsqueeze(0)   # [1, N, D]
        )
        return self.layer_norm(query + attn_output.squeeze(0))


class CombinedModel(nn.Module):
    """多模态融合模型，整合文本、数值、标签和时间特征"""
    
    def __init__(self, use_img: bool, use_text_attributes: bool, 
                 use_label_attributes: bool, use_numeric_attributes: bool,
                 use_temporal_features: bool, embedding_dim: int, 
                 hidden_dim: int, num_heads: int, gpu_num: int, dropout: float):
        super().__init__()
        
        # 参数配置
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.use_img = use_img
        self.use_text_attributes = use_text_attributes
        self.use_label_attributes = use_label_attributes
        self.use_numeric_attributes = use_numeric_attributes
        self.use_temporal_features = use_temporal_features
        
        # 初始化编码器和融合模块
        self._init_encoders(embedding_dim)
        self.cross_attentions = self._build_cross_attention_modules()
        self.final_proj = nn.Linear(embedding_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def _init_encoders(self, embed_dim: int):
        """初始化各模态编码器"""
        self.label_feature_fc = nn.Sequential(
            nn.Linear(9, embed_dim),
            nn.LayerNorm(embed_dim)
        )
        self.numeric_feature_fc = nn.Sequential(
            nn.Linear(4, embed_dim),
            nn.LayerNorm(embed_dim)
        )
        self.temporal_feature_fc = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.LayerNorm(embed_dim)
        )

    def _build_cross_attention_modules(self) -> nn.ModuleDict:
        """动态构建交叉注意力模块"""
        modules = nn.ModuleDict()
        active_modalities = []
        
        # 记录激活的模态
        if self.use_text_attributes: 
            active_modalities.append('text')
        if self.use_label_attributes: 
            active_modalities.append('label')
        if self.use_numeric_attributes: 
            active_modalities.append('numeric')
        if self.use_temporal_features: 
            active_modalities.append('temporal')
        if self.use_img: 
            active_modalities.append('image')
        
        # 构建所有两两交叉注意力组合
        for i in range(len(active_modalities)):
            for j in range(i+1, len(active_modalities)):
                mod1, mod2 = active_modalities[i], active_modalities[j]
                modules[f"{mod1}_{mod2}"] = CrossModalAttention(
                    self.embedding_dim, self.num_heads
                )
        return modules

    def _cross_modal_fusion(self, embeddings: Dict[str, torch.Tensor]) -> torch.Tensor:
        """执行跨模态注意力融合"""
        if not embeddings:
            raise ValueError("embeddings字典不能为空")
            
        fused_feature = torch.zeros_like(next(iter(embeddings.values())))
        count = 0.0
        
        # 遍历所有预定义的交叉注意力组合
        for name in self.cross_attentions:
            mod1, mod2 = name.split('_')
            query = embeddings[mod1]
            key = embeddings[mod2]
            
            # 双向注意力计算
            fusion_1 = self.cross_attentions[name](query, key, key)
            fusion_2 = self.cross_attentions[name](key, query, query)
            
            fused_feature += fusion_1 + fusion_2
            count += 2.0
        
        # 平均融合
        if count > 0:
            return fused_feature / count
        else:
            # 确保返回tensor类型
            return torch.stack(list(embeddings.values())).sum(dim=0)

    def forward(self, numeric_features: torch.Tensor, label_features: torch.Tensor,
               text_embeddings: torch.Tensor, dummy_embeddings: torch.Tensor) -> torch.Tensor:
        """前向传播
        
        Args:
            numeric_features: 数值特征 [batch_size, 4]
            label_features: 标签特征 [batch_size, 9]
            text_embeddings: 文本嵌入 [batch_size, embedding_dim]
            dummy_embeddings: 时间嵌入 [batch_size, embedding_dim]
            
        Returns:
            torch.Tensor: 融合后的特征 [batch_size, hidden_dim]
        """
        # 编码各模态特征
        embeddings = {}
        
        if self.use_text_attributes:
            embeddings['text'] = text_embeddings
            
        if self.use_label_attributes:
            embeddings['label'] = self.label_feature_fc(label_features)
            
        if self.use_numeric_attributes:
            embeddings['numeric'] = self.numeric_feature_fc(numeric_features)
            
        if self.use_temporal_features:
            embeddings['temporal'] = self.temporal_feature_fc(dummy_embeddings)
        
        # 跨模态融合
        if len(embeddings) > 1:
            fused_feature = self._cross_modal_fusion(embeddings)
        else:  # 单模态直接使用
            fused_feature = next(iter(embeddings.values()))
        
        # 最终投影与正则化
        output = self.final_proj(self.dropout(fused_feature))
        return F.tanh(output)


class GatedResidualMLPBlock(nn.Module):
    """门控增强版残差块，集成特征选择机制"""
    
    def __init__(self, input_dim: int, expansion: int = 4, dropout: float = 0.2,
                 num_quantiles: Optional[int] = None):
        super().__init__()
        hidden_dim = input_dim * expansion
        
        # 主残差路径
        self.block = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, input_dim),
            nn.Dropout(dropout)
        )
        
        # 分位数感知门控（可选）
        if num_quantiles:
            self.quant_gate = nn.Linear(num_quantiles, 1, bias=False)
            
        # 门控选择单元
        self.gate_controller = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Linear(input_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # 初始化参数
        self._init_weights()

    def _init_weights(self):
        """初始化门控层权重"""
        nn.init.constant_(self.gate_controller[-2].bias, -2.0)

    def forward(self, x: torch.Tensor, quant_emb: Optional[torch.Tensor] = None) -> torch.Tensor:
        """前向传播
        
        Args:
            x: 输入特征 [batch_size, input_dim]
            quant_emb: 分位数嵌入 (可选)
            
        Returns:
            torch.Tensor: 门控残差输出 [batch_size, input_dim]
        """
        gate = self.gate_controller(x.detach())
        
        # 可选分位数条件门控
        if hasattr(self, 'quant_gate') and quant_emb is not None:
            gate += self.quant_gate(quant_emb)
            
        return x + gate * self.block(x)


class IndependentQuantileHead(nn.Module):
    """独立分位数预测头"""
    
    def __init__(self, hidden_dim: int, quantiles: int):
        super().__init__()
        self.heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.GELU(),
                nn.Linear(hidden_dim // 2, 1)
            ) for _ in range(quantiles)
        ])
        
        # 初始化参数
        for head in self.heads:
            # 获取Sequential中的最后一个Linear层
            if isinstance(head, nn.Sequential):
                last_layer = head[-1]
                if isinstance(last_layer, nn.Linear):
                    nn.init.kaiming_normal_(last_layer.weight, mode='fan_in', nonlinearity='linear')
                    nn.init.constant_(last_layer.bias, 0.0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """前向传播
        
        Args:
            x: 输入特征 [batch_size, hidden_dim]
            
        Returns:
            torch.Tensor: 分位数预测 [batch_size, num_quantiles]
        """
        return torch.cat([head(x) for head in self.heads], dim=1)


class StaticPredictor(nn.Module):
    """静态预测器，用于分位数预测或单点预测"""
    
    def __init__(self, input_dim: int, hidden_dim: int = 64, dropout: float = 0.2,
                 num_blocks: int = 3, num_quantiles: int = 3, output_type: str = 'quantile'):
        super().__init__()
        self.output_type = output_type
        self.num_quantiles = num_quantiles
        self.num_blocks = num_blocks
        # 基础特征提取层
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        # 堆叠门控残差块
        self.gated_blocks = nn.ModuleList([
            GatedResidualMLPBlock(hidden_dim, expansion=4, dropout=dropout, num_quantiles=(num_quantiles if output_type=='quantile' else None))
            for _ in range(num_blocks)
        ])
        if output_type == 'quantile':
            self.quantile_head = IndependentQuantileHead(hidden_dim, num_quantiles)
        else:
            self.value_head = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.GELU(),
                nn.Linear(hidden_dim // 2, 1)
            )
        self.register_buffer('scale', torch.tensor([1e5]))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        features = self.feature_extractor(x)
        # 依次通过门控残差块
        for block in self.gated_blocks:
            features = block(features)
        if self.output_type == 'quantile':
            quantiles = self.quantile_head(features)
            return quantiles
        else:
            value = self.value_head(features)
            return value.squeeze(-1)


class MultiScaleSplit(nn.Module):
    """多尺度分割模块，用于提取不同时间尺度的特征"""
    
    def __init__(self, input_dim: int = 1, scales: List[int] = [3, 5, 7], 
                 hidden_dim: int = 64, output_dim: int = 128,
                 constraint_type: str = 'softmax'):
        super().__init__()
        # 强制scales为奇数
        self.scales = [s if s % 2 == 1 else s + 1 for s in scales]
        
        # 卷积层定义
        self.conv_layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(input_dim, hidden_dim, kernel_size=s, padding=(s-1)//2),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU()
            ) for s in scales
        ])
        
        # 可学习的尺度权重
        self.scale_weights = nn.Parameter(torch.ones(len(scales)))
        self.constraint_type = constraint_type.lower()
        
        # 特征融合层
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * len(scales), output_dim),
            nn.LayerNorm(output_dim),
            nn.Dropout(0.2),
            nn.ReLU()
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """前向传播
        
        Args:
            x: 输入序列 [batch_size, seq_len, input_dim]
            
        Returns:
            torch.Tensor: 多尺度特征 [batch_size, seq_len, output_dim]
        """
        # 输入维度调整
        x = x.permute(0, 2, 1)  # [batch_size, input_dim, seq_len]
        
        # 提取多尺度特征
        features = [conv(x).permute(0, 2, 1) for conv in self.conv_layers]
        
        # 应用权重约束
        if self.constraint_type == 'softmax':
            weights = torch.softmax(self.scale_weights, dim=0)
        elif self.constraint_type == 'sigmoid':
            weights = torch.sigmoid(self.scale_weights)
        else:
            raise ValueError(f"不支持的约束类型: {self.constraint_type}")
        
        # 加权融合
        weighted_features = [
            feat * weight.reshape(1, 1, -1) for feat, weight in zip(features, weights)
        ]
        
        # 拼接与融合
        fused = torch.cat(weighted_features, dim=-1)
        return self.fusion(fused)


class DynamicPredictor(nn.Module):
    """动态预测器，用于时间序列预测"""
    
    def __init__(self, input_dim: int, d_model: int, d_n: int, 
                 current_seq_len: int, predict_seq_len: int,
                 num_heads: int = 1, num_layers: int = 2, dropout: float = 0.1):
        super().__init__()
        self.current_seq_len = current_seq_len
        self.predict_seq_len = predict_seq_len
        self.d_m = d_model
        self.d_n = d_n
        self.scales = [3, 5, 7]
        
        # 多尺度分割模块
        self.multiscale = MultiScaleSplit(
            input_dim=input_dim,
            scales=self.scales,
            hidden_dim=d_n,
            output_dim=d_model
        )
        
        # 共享的Transformer编码器
        self.shared_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=num_heads,
                dim_feedforward=d_n * 2,
                dropout=dropout
            ),
            num_layers=num_layers
        )
        
        # 时间维度转换层
        self.seq_len_adapter = nn.Conv1d(
            in_channels=current_seq_len,
            out_channels=predict_seq_len,
            kernel_size=3,
            padding=1
        )
        
        # 时序演化分支
        self.temporal_proj = nn.Linear(d_model, d_model)
        
        # 跨模态交互分支
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=num_heads,
            dropout=dropout
        )
        self.cross_mlp = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.Dropout(dropout),
            nn.ReLU()
        )
        
        # 自适应融合与输出
        self.fusion_weight = nn.Sequential(
            nn.Linear(d_model, predict_seq_len),
            nn.Sigmoid()
        )
        self.decoder = nn.Linear(d_model, 1)

    def forward(self, seq_input: torch.Tensor, multi_modal: torch.Tensor) -> torch.Tensor:
        """前向传播
        
        Args:
            seq_input: 时序输入 [batch_size, current_seq_len]
            multi_modal: 多模态特征 [batch_size, d_model]
            
        Returns:
            torch.Tensor: 预测序列 [batch_size, predict_seq_len]
        """
        # 维度调整
        seq_input = seq_input.unsqueeze(-1)  # [batch_size, current_seq_len, 1]
        
        # 多尺度特征提取
        ms_feature = self.multiscale(seq_input)  # [batch_size, current_seq_len, d_model]
        
        # Transformer编码
        encoded = self.shared_encoder(ms_feature.permute(1, 0, 2))  # [current_seq_len, batch_size, d_model]
        encoded = encoded.permute(1, 0, 2)  # [batch_size, current_seq_len, d_model]
        
        # 时序分支
        temporal_feat = self.temporal_proj(encoded)  # [batch_size, current_seq_len, d_model]

        # 跨模态交互分支
        global_context = temporal_feat.mean(dim=1)  # [batch_size, d_model]
        cross_feat = self._cross_processing(global_context, multi_modal)  # [batch_size, predict_seq_len, d_model]

        # 自适应融合
        fused = self._adaptive_fusion(temporal_feat, cross_feat)  # [batch_size, predict_seq_len, d_model]

        return self.decoder(fused).squeeze(-1)

    def _cross_processing(self, global_context: torch.Tensor, 
                         multi_modal: torch.Tensor) -> torch.Tensor:
        """基于全局上下文的跨模态注意力"""
        # 查询向量：全局时序特征
        query = global_context.unsqueeze(0).expand(self.predict_seq_len, -1, -1)
        
        # 键值对：多模态特征
        key = value = multi_modal.unsqueeze(0).expand(self.predict_seq_len, -1, -1)
        
        # 注意力计算
        attn_out, _ = self.cross_attn(query, key, value)
        return self.cross_mlp(attn_out.permute(1, 0, 2))

    def _adaptive_fusion(self, temporal: torch.Tensor, cross: torch.Tensor) -> torch.Tensor:
        """时间步粒度的动态融合"""
        # 时序特征映射到预测空间
        temporal_proj = self.seq_len_adapter(temporal)  # [batch_size, predict_seq_len, d_model]
        
        # 生成动态融合权重
        attn_weights = torch.matmul(
            temporal_proj, 
            cross.transpose(1, 2)
        ) / np.sqrt(self.d_m)
        attn_weights = torch.softmax(attn_weights, dim=-1)
        
        # 特征融合
        weighted_temporal = torch.matmul(attn_weights, temporal_proj)
        fused = weighted_temporal + cross
        
        return fused
    
    def _generate_position_query(self, device: torch.device, batch_size: int) -> torch.Tensor:
        """生成预测位置查询向量"""
        pos = torch.arange(self.predict_seq_len, device=device).float()
        pos_embed = pos.unsqueeze(1) * nn.Parameter(
            torch.randn(self.predict_seq_len, self.d_m, device=device)
        )
        return pos_embed.unsqueeze(1).expand(-1, batch_size, -1)


class WXM(nn.Module):
    """多模态活动预测模型"""
    
    def __init__(self, args, num_quantiles: int = 3):
        super().__init__()
        self.config = self._create_config_from_args(args)
        self._init_encoders()
        # 新增：静态预测输出类型
        self.static_output_type = getattr(args, 'static_output_type', 'quantile')
        self._init_predictors(num_quantiles)
    
    def _create_config_from_args(self, args) -> ModelConfig:
        """从参数对象创建配置"""
        return ModelConfig(
            embedding_dim=args.embedding_dim,
            hidden_dim=args.d_model,
            d_model=args.d_model,
            d_s=args.d_s,
            d_n=args.d_n,
            num_heads=args.num_attn_heads,
            num_layers=args.num_hidden_layers,
            current_seq_len=args.current_seq_len,
            predict_seq_len=args.predict_seq_len,
            total_seq_len=args.total_seq_len,
            input_dim=args.input_dim,
            use_img=args.use_img,
            use_text_attributes=args.use_text_attributes,
            use_label_attributes=args.use_label_attributes,
            use_numeric_attributes=args.use_numeric_attributes,
            use_temporal_features=args.use_temporal_features,
            use_current_seq=args.use_current_seq,
            use_similar=args.use_similar,
            use_encoder_mask=args.use_encoder_mask,
            autoregressive=args.autoregressive,
            dropout=args.dropout,
            gpu_num=args.gpu_num,
            task_type=args.task_type
        )
    
    def _init_encoders(self):
        """初始化各模态编码器"""
        # 文本编码器
        if self.config.use_text_attributes:
            self.text_encoder = TextEmbedder(
                embedding_dim=self.config.embedding_dim,
                gpu_num=self.config.gpu_num,
                dropout=self.config.dropout
            )
        
        # 图像编码器
        if self.config.use_img:
            self.img_encoder = ImageEmbedder()
        
        # 时间特征编码器
        if self.config.use_temporal_features:
            self.temporal_encoder = DummyEmbedder(
                embedding_dim=self.config.embedding_dim,
                dropout=self.config.dropout
            )
        
        # 多模态融合模块
        self.multimodal_fusion = CombinedModel(
            use_img=self.config.use_img,
            use_text_attributes=self.config.use_text_attributes,
            use_label_attributes=self.config.use_label_attributes,
            use_numeric_attributes=self.config.use_numeric_attributes,
            use_temporal_features=self.config.use_temporal_features,
            embedding_dim=self.config.embedding_dim,
            hidden_dim=self.config.d_model,
            num_heads=self.config.num_heads,
            gpu_num=self.config.gpu_num,
            dropout=self.config.dropout
        )
    
    def _init_predictors(self, num_quantiles: int):
        # 静态预测器
        if self.static_output_type == 'quantile':
            self.static_predictor = StaticPredictor(
                input_dim=self.config.d_model,
                hidden_dim=self.config.d_s,
                dropout=self.config.dropout,
                num_blocks=getattr(self.config, 'num_gated_blocks', 2),
                num_quantiles=num_quantiles,
                output_type='quantile'
            )
        else:
            self.static_predictor = StaticPredictor(
                input_dim=self.config.d_model,
                hidden_dim=self.config.d_s,
                dropout=self.config.dropout,
                num_blocks=getattr(self.config, 'num_gated_blocks', 2),
                num_quantiles=1,
                output_type='value'
            )
        # 动态预测器
        self.dynamic_predictor = DynamicPredictor(
            input_dim=self.config.input_dim,
            d_model=self.config.d_model,
            d_n=self.config.d_n,
            current_seq_len=self.config.current_seq_len,
            predict_seq_len=self.config.predict_seq_len,
            num_heads=self.config.num_heads,
            num_layers=self.config.num_layers,
            dropout=self.config.dropout
        )
    
    def forward(self, input_sequence: torch.Tensor, numeric_features: torch.Tensor,
                label_features: torch.Tensor, temporal_features: torch.Tensor,
                activity_text: Dict[str, List[str]], 
                similar_sequences: Optional[torch.Tensor] = None) -> torch.Tensor:
        multimodal_features = self._encode_multimodal_features(
            numeric_features, label_features, temporal_features, activity_text
        )
        if self.config.task_type == 'static':
            return self.static_predictor(multimodal_features)
        else:
            return self.dynamic_predictor(input_sequence, multimodal_features)
    
    def _encode_multimodal_features(self, numeric_features: torch.Tensor,
                                  label_features: torch.Tensor,
                                  temporal_features: torch.Tensor,
                                  activity_text: Dict[str, List[str]]) -> torch.Tensor:
        """编码多模态特征"""
        # 时间特征编码
        temporal_embeddings = None
        if self.config.use_temporal_features:
            temporal_embeddings = self.temporal_encoder(temporal_features)
        
        # 文本特征编码
        text_embeddings = None
        if self.config.use_text_attributes:
            text_embeddings = self.text_encoder(
                activity_text['activity_name'],
                activity_text['activity_title'],
                activity_text['product_names']
            )
        
        # 多模态融合
        return self.multimodal_fusion(
            numeric_features, label_features, text_embeddings, temporal_embeddings
        )




===============================================
3. DDN归一化模块 - models/DDN.py
===============================================
功能：DDN归一化模块，用于动态预测的数据归一化处理


import torch
import torch.nn as nn
import copy
import torch.nn.functional as F
import numpy as np
from pytorch_wavelets import DWT1DInverse, DWT1DForward
from statsmodels.tsa.stattools import adfuller

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.ADF import ad_fuller as adf
from utils.learnable_wavelet import DWT1D
# 魔改版DDN模型，用于实现自适应归一化
# DDN对模型输入序列X进行归一化，对模型输出序列Y进行反归一化
def gaussian_weight(size, sigma=1):
    coords = torch.arange(size, dtype=torch.float32) - size // 2
    kernel = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
    kernel /= kernel.sum()
    return kernel


def exponential_smoothing(data, alpha):
    smoothed_data = data.clone()
    for t in range(1, data.shape[-1]):
        smoothed_data[..., t] = alpha * data[..., t] + (1 - alpha) * smoothed_data[..., t - 1]
    return smoothed_data


class DDN(nn.Module):
    def __init__(self, configs):
        super(DDN, self).__init__()
        self.configs = configs
        self.seq_len = configs.current_seq_len
        self.pred_len = configs.predict_seq_len
        self.kernel = kernel = configs.kernel_len
        self.hkernel = hkernel = configs.hkernel_len
        self.pad = nn.ReplicationPad1d(padding=(kernel // 2, kernel // 2 - ((kernel + 1) % 2)))
        if hkernel is not None:
            self.hpad = nn.ReplicationPad1d(padding=(hkernel // 2, hkernel // 2 - ((hkernel + 1) % 2)))
        self.channels = 1
        self.station_type = configs.station_type

        self.seq_len_new = self.seq_len
        self.pred_len_new = self.pred_len
        self.epsilon = 1e-5
        self._build_model()

    def _build_model(self):
        args = copy.deepcopy(self.configs)
        args.current_seq_len = self.configs.current_seq_len
        args.predict_seq_len = self.configs.predict_seq_len
        args.moving_avg = 3
        self.norm_func = self.norm_sliding

        wave = self.configs.wavelet
        wave_dict = {'coif6': 17, 'coif3': 8, 'sym3': 2}
        self.len, self.j = wave_dict[wave], self.configs.j
        self.dwt = DWT1D(wave=wave, J=self.j, learnable=args.learnable)
        self.dwt_ratio = nn.Parameter(
            torch.clamp(torch.full((1, self.channels, 1), 0.), min=0., max=1.)
        )
        self.mlp = Statics_MLP(
            self.configs.current_seq_len, args.pd_model, args.pd_ff,
            self.configs.predict_seq_len, drop_rate=args.dr, layer=args.pe_layers
        )

    def normalize(self, x, p_value=True):
        if self.station_type == 'adaptive':
            # 1. 交换维度：(batch_size, seq_len, channels) -> (batch_size, channels, seq_len)
            x_transposed = x.transpose(-1, -2)
            
            # 2. 对交换后的 x 进行归一化,得到归一化后的输入、序列统计量和预测的统计量
            # seq_ms不为空
            # pred_ms为nan值
            norm_input, seq_ms, pred_ms = self.norm(x_transposed)
            # 3. 将预测的均值和标准差拼接在一起
            outputs = torch.cat(pred_ms, dim=1)
            # 4. 交换维度：(batch_size, channels, pred_len) -> (batch_size, pred_len, channels)
            outputs_transposed = outputs.transpose(-1, -2)
            
            # 5. 返回归一化后的输入、预测的统计量和序列统计量(元组)
            # outputs_transposed 的维度设置为 (batch_size, pred_len, 2) 是为了与模型的输出保持一致，并且存储预测序列的均值和标准差。
            return norm_input.transpose(-1, -2), outputs_transposed, seq_ms
        else:
            # 如果不使用自适应归一化，直接返回原始输入
            return x, None

    def de_normalize(self, input, station_pred):
        if self.station_type == 'adaptive':
            bs, l, dim = input.shape
            mean = station_pred[..., :station_pred.shape[-1] // 2]
            std = station_pred[..., station_pred.shape[-1] // 2:]
            output = input * (std + self.epsilon) + mean
            return output.reshape(bs, l, dim)
        else:
            return input

# 输入X的维度为(batch_size, channels, seq_len)
# 条件返回值说明
# predict=True 且 self.j > 0	norm_x, (seq_m, seq_s), (pred_m, pred_s)	包含时域和频域归一化后的序列、历史统计量、预测的未来统计量。
# predict=True 且 self.j <= 0	norm_x, (seq_m, seq_s), (mov_m, mov_s)	仅包含时域归一化后的序列、历史统计量、预测的未来统计量。
# predict=False	norm_x, (seq_m, seq_s)	仅包含时域归一化后的序列和历史统计量，不预测未来统计量。
    def norm(self, x, predict=True):
        norm_x, (seq_m, seq_s) = self.norm_func(x)
        # norm_x: 归一化后的输入序列 [batch_size, channels, seq_len]
        # seq_m: 输入序列的均值 [batch_size, channels, seq_len]
        # seq_s: 输入序列的标准差 [batch_size, channels, seq_len]
        # print("norm norm_x shape",norm_x.shape)
        # print("norm seq_m shape",seq_m.shape)
        # print("norm seq_s shape",seq_s.shape)
        if predict is True:
            mov_m, mov_s = self.mlp(seq_m, seq_s, x)
            if self.j > 0:
                ac, dc_list = self.dwt(x)
                norm_ac, (mac, sac) = self.norm_func(ac, kernel=self.hkernel)
                norm_dc, m_list, s_list = [], [], []
                for i, dc in enumerate(dc_list):
                    dc, (mdc, sdc) = self.norm_func(dc, kernel=self.hkernel)
                    norm_dc.append(dc)
                    m_list.append(mdc)
                    s_list.append(sdc)

                pred_m, pred_s = self.mlp(
                    self.dwt([mac, m_list], 1),
                    self.dwt([sac, s_list], 1), self.dwt([ac, dc_list], 1))

                dwt_r, mov_r = self.dwt_ratio, 1 - self.dwt_ratio
                norm_x = norm_x * mov_r + self.dwt([norm_ac, norm_dc], 1) * dwt_r
                pred_m = mov_m * mov_r + pred_m * dwt_r
                pred_s = mov_s * mov_r + pred_s * dwt_r
                return norm_x, (seq_m, seq_s), (pred_m, pred_s)
            else:
                return norm_x, (seq_m, seq_s), (mov_m, mov_s)
        return norm_x, (seq_m, seq_s)

    def norm_sliding(self, x, kernel=None):
        if kernel is None:
            kernel, pad = self.kernel, self.pad
        else:
            pad = self.hpad
        x_window = x.unfold(-1, kernel, 1)  # sliding window
        m, s = x_window.mean(dim=-1), x_window.std(dim=-1)  # acquire sliding mean and sliding standard deviation
        m, s = pad(m), pad(s)  # nn.ReplicationPad1d(padding=(kernel // 2, kernel // 2 - ((kernel + 1) % 2)))
        x = (x - m) / (s + self.epsilon)  # x is stationary series
        return x, (m, s)  # m, s are non-stationary factors

    def p_value(self, x, float_type=torch.float32):
        B, ch, dim = x.shape
        p_value = adf(x.reshape(-1, dim), maxlag=min(self.kernel, 24), float_type=float_type).view(B, ch, 1)
        return p_value


class FFN(nn.Module):
    def __init__(self, d_model, d_ff, activation, drop_rate=0.1, bias=False):
        super(FFN, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(d_model, d_ff, bias=bias), activation,
            nn.Linear(d_ff, d_model, bias=bias), nn.Dropout(drop_rate),
        )

    def forward(self, x):
        x = self.mlp(x)
        return x

# Statics_MLP学习历史数据的统计特性，预测未来时间段的均值和标准差，用于对模型输出进行反归一化

# 问题：是对未来时间预测的均值和标准差值大小都差不多
class Statics_MLP(nn.Module):
    def __init__(self, seq_len, d_model, d_ff,
                 pred_len, drop_rate=0.1, bias=False, layer=1):
        super(Statics_MLP, self).__init__()
        project = nn.Sequential(nn.Linear(seq_len, d_model, bias=bias), nn.Dropout(drop_rate))
        self.m_project, self.s_project = copy.deepcopy(project), copy.deepcopy(project)
        self.mean_proj, self.std_proj = copy.deepcopy(project), copy.deepcopy(project)

        self.m_concat = nn.Sequential(nn.Linear(d_model * 2, d_model), nn.Dropout(drop_rate))
        self.s_concat = nn.Sequential(nn.Linear(d_model * 2, d_model), nn.Dropout(drop_rate))

        ffn = nn.Sequential(*[FFN(d_model, d_ff, nn.LeakyReLU(), drop_rate, bias) for _ in range(layer)])
        self.mean_ffn, self.std_ffn = copy.deepcopy(ffn), copy.deepcopy(ffn)

        self.mean_pred = nn.Linear(d_model, pred_len, bias=bias)
        self.std_pred = nn.Linear(d_model, pred_len, bias=bias)

    def forward(self, mean, std, x=None, x2=None):
        m_all, s_all = mean.mean(dim=-1, keepdim=True), std.mean(dim=-1, keepdim=True)
        mean_r, std_r = mean - m_all, std - s_all
        mean_r, std_r = self.mean_proj(mean_r), self.std_proj(std_r)
        if x is not None:
            m_orig, s_ori = self.m_project(x - m_all), \
                self.s_project(x if x2 is None else x2 - s_all)
            mean_r, std_r = self.m_concat(torch.cat([m_orig, mean_r], dim=-1)), \
                self.s_concat(torch.cat([s_ori, std_r], dim=-1))

        mean_r, std_r = self.mean_ffn(mean_r), self.std_ffn(std_r)
        mean_r, std_r = self.mean_pred(mean_r), self.std_pred(std_r)

        mean, std = mean_r + m_all, std_r + s_all
        return mean, F.relu(std)


def normalization(x, mean=None, std=None):
    if mean is not None and std is not None:
        return (x - mean) / std
    mean = x.mean(-1, keepdim=True).detach()
    x = x - mean
    std = torch.sqrt(torch.var(x, dim=-1, keepdim=True, unbiased=False) + 1e-5)
    x /= std
    return x, mean, std



===============================================
4. 数据处理模块 - utils/data_processor.py
===============================================
功能：数据预处理、特征工程、数据集创建


"""
统一的数据处理模块
整合所有数据处理功能，避免代码重复
"""
import os
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from torch.nn.utils.rnn import pad_sequence
import pickle
from typing import Dict, List, Tuple, Optional


class DataProcessor:
    """统一的数据处理器"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.scalers = {}
        self.label_encoders = {}
        
    def process_activities(self, original_data_path: str, historical_activities_path: str, freq: str) -> None:
        """处理活动序列数据，生成活动统计数据"""
        print(f"开始处理活动数据，频率: {freq}")
        
        activities_duration = []
        processed_files_count = 0
        total_files = len([f for f in os.listdir(original_data_path) if f.endswith('.csv')])
        print(f"总共发现 {total_files} 个CSV文件")
        
        for i, file_name in enumerate(os.listdir(original_data_path)):
            if not file_name.endswith('.csv'):
                continue
                
            activity_id = file_name[:-4]
            file_path = os.path.join(original_data_path, file_name)
            
            # 检查文件大小
            file_size = os.path.getsize(file_path)
            print(f"[{i+1}/{total_files}] 处理文件: {file_name} (大小: {file_size/1024/1024:.2f}MB)")
            
            # 跳过过大的文件（超过100MB）
            if file_size > 100 * 1024 * 1024:  # 100MB
                print(f"文件过大，跳过: {file_name}")
                continue
            
            # 尝试多种方式读取CSV文件
            data = None
            try:
                # 首先尝试标准方式读取
                data = pd.read_csv(file_path)
                print(f"  成功读取文件: {file_name}")
            except Exception as e1:
                print(f"  标准读取失败，尝试使用python引擎: {file_name}")
                try:
                    # 尝试使用python引擎
                    data = pd.read_csv(file_path, engine='python')
                    print(f"  Python引擎读取成功: {file_name}")
                except Exception as e2:
                    print(f"  Python引擎读取失败，尝试指定编码: {file_name}")
                    try:
                        # 尝试指定编码
                        data = pd.read_csv(file_path, engine='python', encoding='utf-8')
                        print(f"  UTF-8编码读取成功: {file_name}")
                    except Exception as e3:
                        print(f"  UTF-8编码读取失败，尝试GBK编码: {file_name}")
                        try:
                            # 尝试GBK编码
                            data = pd.read_csv(file_path, engine='python', encoding='gbk')
                            print(f"  GBK编码读取成功: {file_name}")
                        except Exception as e4:
                            print(f"  所有读取方式都失败，跳过文件: {file_name}")
                            print(f"  错误信息: {e4}")
                            continue
            
            if data is None or data.empty:
                print(f"  文件 {file_name} 数据为空，跳过处理")
                continue
                
            # 检查必要的列是否存在
            required_columns = ['create_date', 'activity_name']
            missing_columns = [col for col in required_columns if col not in data.columns]
            if missing_columns:
                print(f"  文件 {file_name} 缺少必要列: {missing_columns}，跳过处理")
                continue
            
            try:
                data['create_date'] = pd.to_datetime(data['create_date'])
                print(f"  日期转换成功: {file_name}")
            except Exception as e:
                print(f"  转换日期失败: {file_name}, 错误: {e}")
                continue
            
            if data['activity_name'].isnull().all():
                print(f"  文件 {file_name} 活动名称为空，跳过处理")
                continue
                
            activity_name = data['activity_name'].iloc[1] if len(data) > 1 else data['activity_name'].iloc[0]
            
            # 按频率聚合数据
            try:
                processed_data = self._aggregate_by_freq(data, freq)
                print(f"  数据聚合成功: {file_name}")
            except Exception as e:
                print(f"  聚合数据失败: {file_name}, 错误: {e}")
                continue
            
            # 保存处理后的数据
            processed_data_path = f"{original_data_path}_{freq}"
            os.makedirs(processed_data_path, exist_ok=True)
            output_file_path = os.path.join(processed_data_path, f"{activity_id}_{freq}.csv")
            processed_data.to_csv(output_file_path, index=False)
            
            # 计算统计信息
            oc_sum = processed_data['oc'].sum()
            user_sum = processed_data['uc'].sum()
            duration = (processed_data['oc'] > 0).sum()
            
            activities_duration.append({
                'activity_id': activity_id,
                'activity_name': activity_name,
                'duration': duration,
                'oc_sum': oc_sum,
                'user_sum': user_sum
            })
            processed_files_count += 1
            print(f"  处理完成: {file_name}")
            
        # 保存活动持续时间数据
        if activities_duration:
            activities_duration_df = pd.DataFrame(activities_duration)
            activities_duration_df.to_csv(
                os.path.join(historical_activities_path, f'activities_duration_{freq}.csv'), 
                index=False
            )
            print(f"成功处理 {processed_files_count} 个文件")
        else:
            print("没有成功处理任何文件")
    
    def _aggregate_by_freq(self, data: pd.DataFrame, freq: str) -> pd.DataFrame:
        """按频率聚合数据"""
        data.set_index('create_date', inplace=True)
        
        freq_mapping = {
            'h': ('H', 'H'),
            'd': ('D', 'D'), 
            'w': ('W', 'W'),
            'm': ('ME', 'ME')
        }
        
        if freq not in freq_mapping:
            raise ValueError(f"不支持的频率: {freq}")
            
        resample_freq, range_freq = freq_mapping[freq]
        
        processed_data = data.resample(resample_freq).agg({
            'id': 'size', 
            'user_id': pd.Series.nunique
        }).reset_index()
        
        # 补齐缺失日期
        full_range = pd.date_range(
            start=processed_data['create_date'].min(),
            end=processed_data['create_date'].max(), 
            freq=range_freq
        )
        processed_data = processed_data.set_index('create_date').reindex(
            full_range, fill_value=0
        ).rename_axis('create_date').reset_index()
        
        processed_data.rename(columns={'id': 'oc', 'user_id': 'uc'}, inplace=True)
        return processed_data
    
    def create_dataset(self, task_type: str, **kwargs) -> pd.DataFrame:
        """创建数据集"""
        if task_type == 'static':
            return self._create_static_dataset(**kwargs)
        elif task_type == 'dynamic':
            return self._create_dynamic_dataset(**kwargs)
        else:
            raise ValueError(f"不支持的任务类型: {task_type}")
    
    def _create_static_dataset(self, **kwargs) -> pd.DataFrame:
        """创建静态预测数据集（迁移自 enrich_dataset_with_attributes 相关逻辑）"""
        # 1. 先用 batch_process_sequences 生成活动ID列表
        task_type = 'static'
        sequence_process_type = kwargs.get('sequence_process_type', 'LTTB')
        current_days = kwargs.get('current_seq_len', 14)
        predict_days = kwargs.get('predict_seq_len', 7)
        total_days = kwargs.get('total_seq_len', 30)
        oringinal_data_path = kwargs.get('oringinal_data_path', self.config['data']['original_data_path'])
        historical_activities_path = kwargs.get('historical_activities_path', self.config['data']['historical_activities_path'])
        target = kwargs.get('target', self.config['prediction']['target'])
        freq = kwargs.get('freq', self.config['prediction']['freq'])
        apply_diff = kwargs.get('apply_diff', self.config['sequence']['apply_diff'])
        apply_smoothing = kwargs.get('apply_smoothing', self.config['sequence']['apply_smoothing'])
        activities_attributes_path = kwargs.get('activities_attributes_path', self.config['data']['activities_attributes_path'])

        # 只生成活动ID
        processed_data_path_with_freq = f"{oringinal_data_path}_{freq}"
        result_dataset = pd.DataFrame()
        historical_activities = pd.read_csv(historical_activities_path + f'/activities_duration_{freq}.csv')
        for activity_id in historical_activities['activity_id'].unique():
            encoded_df = pd.DataFrame({'activity_id': [activity_id]})
            result_dataset = pd.concat([result_dataset, encoded_df], ignore_index=True)
        # enrich_dataset_with_attributes
        activity_duration = pd.read_csv(historical_activities_path + f'/activities_duration_{freq}.csv')
        attributes_file = os.path.join(activities_attributes_path, 'all_activities_attributes_threshold.csv')
        activity_attributes = pd.read_csv(attributes_file)
        # 去除重复列
        for col in ['oc_sum', 'user_sum', 'duration']:
            if col in activity_attributes.columns:
                activity_attributes = activity_attributes.drop(columns=[col])
        activity_attributes = pd.merge(activity_attributes, activity_duration[['activity_id', 'oc_sum', 'user_sum','duration']], left_on='id', right_on='activity_id', how='left').drop(columns=['activity_id'])
        enriched_dataset = pd.merge(result_dataset, activity_attributes, left_on='activity_id', right_on='id', how='left').drop(columns=['id'])
        # 排序（如果有 activity_start_time）
        if 'activity_start_time' in enriched_dataset.columns:
            enriched_dataset = enriched_dataset.sort_values(by='activity_start_time')
        return enriched_dataset

    def _create_dynamic_dataset(self, **kwargs) -> pd.DataFrame:
        """创建动态预测数据集（迁移自 batch_process_sequences + enrich_dataset_with_attributes 逻辑）"""
        task_type = 'dynamic'
        sequence_process_type = kwargs.get('sequence_process_type', 'LTTB')
        current_days = kwargs.get('current_seq_len', 14)
        predict_days = kwargs.get('predict_seq_len', 7)
        total_days = kwargs.get('total_seq_len', 30)
        oringinal_data_path = kwargs.get('oringinal_data_path', self.config['data']['original_data_path'])
        historical_activities_path = kwargs.get('historical_activities_path', self.config['data']['historical_activities_path'])
        target = kwargs.get('target', self.config['prediction']['target'])
        freq = kwargs.get('freq', self.config['prediction']['freq'])
        apply_diff = kwargs.get('apply_diff', self.config['sequence']['apply_diff'])
        apply_smoothing = kwargs.get('apply_smoothing', self.config['sequence']['apply_smoothing'])
        activities_attributes_path = kwargs.get('activities_attributes_path', self.config['data']['activities_attributes_path'])

        processed_data_path_with_freq = f"{oringinal_data_path}_{freq}"
        result_dataset = pd.DataFrame()
        historical_activities = pd.read_csv(historical_activities_path + f'/activities_duration_{freq}.csv')
        for activity_id in historical_activities['activity_id'].unique():
            file_path = os.path.join(processed_data_path_with_freq, f"{activity_id}_{freq}.csv")
            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                activity_data = pd.read_csv(file_path)
                activity_length = activity_data.shape[0]
                if activity_length < current_days + predict_days:
                    continue
                sequence_array = activity_data[target].to_numpy().astype(float)
                encoded_df = pd.DataFrame(sequence_array[:current_days + predict_days].reshape(1, -1))
                encoded_df['activity_id'] = activity_id
                result_dataset = pd.concat([result_dataset, encoded_df], ignore_index=True)
            else:
                continue
        # enrich_dataset_with_attributes
        activity_duration = pd.read_csv(historical_activities_path + f'/activities_duration_{freq}.csv')
        attributes_file = os.path.join(activities_attributes_path, 'all_activities_attributes_threshold.csv')
        activity_attributes = pd.read_csv(attributes_file)
        for col in ['oc_sum', 'user_sum', 'duration']:
            if col in activity_attributes.columns:
                activity_attributes = activity_attributes.drop(columns=[col])
        activity_attributes = pd.merge(activity_attributes, activity_duration[['activity_id', 'oc_sum', 'user_sum','duration']], left_on='id', right_on='activity_id', how='left').drop(columns=['activity_id'])
        enriched_dataset = pd.merge(result_dataset, activity_attributes, left_on='activity_id', right_on='id', how='left').drop(columns=['id'])
        if 'activity_start_time' in enriched_dataset.columns:
            enriched_dataset = enriched_dataset.sort_values(by='activity_start_time')
        return enriched_dataset


class UnifiedDataset(Dataset):
    """统一的数据集类，支持静态和动态预测"""
    
    def __init__(self, data_df: pd.DataFrame, config: Dict, task_type: str = 'static', 
                 mode: str = 'train'):
        self.data_df = data_df.copy()
        self.config = config
        self.task_type = task_type
        self.mode = mode
        self.label_encoders = {}  # 添加缺失的属性
        
        # 根据任务类型设置序列长度
        if task_type == 'static':
            self.sequence_len = config['sequence']['total_seq_len']
            self.current_seq_len = config['sequence']['total_seq_len']
        else:  # dynamic
            self.sequence_len = (config['sequence']['current_seq_len'] + 
                               config['sequence']['predict_seq_len'])
            self.current_seq_len = config['sequence']['current_seq_len']
        
        self.preprocess_data()
    
    def __len__(self):
        return len(self.data_df)
    
    def __getitem__(self, idx):
        row = self.data_df.iloc[idx]
        
        if self.task_type == 'dynamic':
            # 动态预测：分离输入序列和目标序列
            # 确保数据类型正确
            input_values = row.iloc[:self.current_seq_len].values
            target_values = row.iloc[self.current_seq_len:self.sequence_len].values
            
            # 转换为float32类型
            try:
                input_sequence = torch.tensor(input_values.astype(np.float32), dtype=torch.float32)
                target_sequence = torch.tensor(target_values.astype(np.float32), dtype=torch.float32)
            except (ValueError, TypeError) as e:
                print(f"数据类型转换错误: {e}")
                print(f"输入序列数据类型: {input_values.dtype}")
                print(f"目标序列数据类型: {target_values.dtype}")
                # 使用默认值
                input_sequence = torch.zeros(self.current_seq_len, dtype=torch.float32)
                target_sequence = torch.zeros(self.sequence_len - self.current_seq_len, dtype=torch.float32)
        else:
            # 静态预测：不需要序列数据
            input_sequence = torch.tensor([])
            target_sequence = torch.tensor([])
        
        # 提取特征
        numeric_features = self._extract_numeric_features(row)
        label_features = self._extract_label_features(row)
        temporal_features = self._extract_temporal_features(row)
        activity_text = self._extract_text_features(row)
        similar_sequences = self._extract_similar_sequences(row)
        
        # 目标值
        if self.task_type == 'static':
            target = torch.tensor(row[self.config['prediction']['static_target']], dtype=torch.float32)
        else:
            target = torch.tensor(0.0)  # 动态预测不需要标量目标
        
        return (input_sequence, target_sequence, target, numeric_features, 
                label_features, temporal_features, activity_text, similar_sequences)
    
    def _extract_numeric_features(self, row) -> torch.Tensor:
        """提取数值特征"""
        return torch.tensor([
            row['activity_budget'],
            row['max_reward_count'], 
            row['min_reward_count'],
            row['duration']
        ], dtype=torch.float32)
    
    def _extract_label_features(self, row) -> torch.Tensor:
        """提取标签特征"""
        return torch.tensor([
            row['customer_id_encoded'],
            row['activity_type_encoded'],
            row['activity_form_encoded'],
            row['bank_name_encoded'],
            row['location_encoded'],
            row['main_reward_type_encoded'],
            row['secondary_reward_type_encoded'],
            row['template_id_encoded'],
            row.get('threshold', 0)  # 兼容性处理
        ], dtype=torch.float32)
    
    def _extract_temporal_features(self, row) -> torch.Tensor:
        """提取时间特征"""
        return torch.tensor(
            row[['day', 'week', 'month', 'year']].astype(float).values,
            dtype=torch.float32
        )
    
    def _extract_text_features(self, row) -> Dict[str, str]:
        """提取文本特征"""
        return {
            'activity_name': row['activity_name'],
            'activity_title': row['activity_title'],
            'product_names': row['product_names']
        }
    
    def _extract_similar_sequences(self, row) -> torch.Tensor:
        """提取相似序列"""
        similar_sequences = []
        for i in range(self.sequence_len):
            similar_column = f'similar_{i}'
            if similar_column in row:
                similar_sequences.append(row[similar_column])
            else:
                similar_sequences.append(np.nan)
        return torch.tensor(similar_sequences, dtype=torch.float32)
    
    def preprocess_data(self):
        """数据预处理"""
        # 异常值处理
        if self.task_type == 'static':
            self._remove_outliers()
        
        # 特征编码
        self._encode_categorical_features()
        
        # 序列处理
        if self.task_type == 'dynamic' and self.config['sequence']['series_scale']:
            self._add_similar_sequences()
            self._scale_sequences()
        
        # 特征归一化
        if self.config['sequence']['feature_scale']:
            self._scale_features()
    
    def _remove_outliers(self):
        """去除异常值"""
        columns_of_interest = [
            'max_reward_count', 'min_reward_count',
            'activity_budget', 'duration', 'oc_sum', 'user_sum'
        ]
        self.data_df = self._remove_outliers_zscore(self.data_df, columns_of_interest)
    
    def _encode_categorical_features(self):
        """编码分类特征"""
        categorical_columns = [
            'customer_id', 'template_id', 'activity_type', 'activity_form',
            'bank_name', 'location', 'main_reward_type', 'secondary_reward_type'
        ]
        
        for col in categorical_columns:
            if col in self.data_df.columns:
                le = LabelEncoder()
                self.data_df[f'{col}_encoded'] = le.fit_transform(self.data_df[col])
                self.label_encoders[col] = le
    
    def _add_similar_sequences(self):
        """添加相似序列"""
        self.data_df = self._find_similar_sequences(self.data_df, self.sequence_len)
    
    def _scale_sequences(self):
        """序列归一化"""
        time_series_data = self.data_df.iloc[:, :self.sequence_len]
        for i in range(len(self.data_df)):
            sequence_scaler = StandardScaler()
            scaled_sequence = sequence_scaler.fit_transform(
                time_series_data.iloc[i, :self.current_seq_len].values.reshape(-1, 1)
            ).flatten()
            self.data_df.iloc[i, :self.current_seq_len] = scaled_sequence
    
    def _scale_features(self):
        """特征归一化"""
        scaler = StandardScaler()
        numeric_columns = [
            'activity_budget', 'max_reward_count', 'min_reward_count', 'duration'
        ]
        self.data_df[numeric_columns] = scaler.fit_transform(self.data_df[numeric_columns])
    
    @staticmethod
    def _remove_outliers_zscore(data: pd.DataFrame, columns: List[str], threshold: float = 3) -> pd.DataFrame:
        """使用Z-score去除异常值"""
        data_clean = data.copy()
        for col in columns:
            if col in data_clean.columns:
                z_scores = np.abs((data_clean[col] - data_clean[col].mean()) / data_clean[col].std())
                data_clean = data_clean[z_scores < threshold]
        return data_clean
    
    @staticmethod
    def _find_similar_sequences(df: pd.DataFrame, sequence_len: int) -> pd.DataFrame:
        """找到相似序列"""
        df = df.reset_index(drop=True)
        sequence_columns = df.columns[:sequence_len]
        
        similar_data = pd.DataFrame(
            index=df.index, 
            columns=[f'similar_{i}' for i in range(sequence_len)]
        )
        similar_data = similar_data.fillna(np.nan)
        
        for row_index in range(len(df)):
            similarities = cosine_similarity(
                df[sequence_columns], 
                df.loc[row_index, sequence_columns].values.reshape(1, -1)
            )
            similarities[row_index] = -1
            similar_row_index = np.argmax(similarities)
            similar_sequence = df.iloc[similar_row_index, :sequence_len].values
            similar_data.iloc[row_index, :sequence_len] = similar_sequence
        
        return pd.concat([df, similar_data], axis=1) 


===============================================
5. 静态预测实验 - utils/static_exp.py
===============================================
功能：静态预测实验执行，支持分位数预测


import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from models.WXM import WXM
import random
import numpy as np
import os
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter

class QuantileLoss(nn.Module):
    def __init__(self, quantiles=[0.1, 0.5, 0.9]):
        super().__init__()
        self.quantiles = torch.tensor(quantiles)
        
    def forward(self, preds, target):
        """
        preds: [batch_size, num_quantiles] 
        target: [batch_size]
        """
        # 显式扩展target维度
        target = target.repeat_interleave(len(self.quantiles)).view(-1, len(self.quantiles))  # [B, Q]
        
        # 逐元素计算误差
        errors = target - preds  # [B, Q]
        
        # 分位数损失计算
        losses = []
        for i, q in enumerate(self.quantiles):
            q_tensor = torch.full_like(errors[:, i], q)  # 显式分位数参数
            loss = torch.max( (q_tensor-1)*errors[:, i], q_tensor*errors[:, i] )
            losses.append(loss)
        
        # 合并损失并平均
        total_loss = torch.stack(losses, dim=1).mean()  # [B,Q] → scalar
        return total_loss

class StaticExp:
    def __init__(self, args):
        self.args = args
        self.device = torch.device(f'cuda:{args.gpu_num}' if torch.cuda.is_available() else 'cpu')
        self.results_dir = 'results/static'
        self.results_file = os.path.join(self.results_dir, 'static_results.txt')
        self.model_save_dir = 'models'
        self.model_save_path = os.path.join(self.model_save_dir, f"{args.model_type}_{args.task_type}.pth")
        
        # 新增：根据参数决定分位数或单点预测
        self.static_output_type = getattr(args, 'static_output_type', 'quantile')
        if self.static_output_type == 'quantile':
            self.quantiles = [0.1, 0.5, 0.9]
            self.model = WXM(args, num_quantiles=len(self.quantiles)).to(self.device)
            self.criterion = self.get_loss_function("quantile")
        else:
            self.quantiles = None
            self.model = WXM(args, num_quantiles=1).to(self.device)
            self.criterion = self.get_loss_function(args.loss)
        
        self.optimizer = optim.Adam(self.model.parameters(), lr=args.static_learning_rate)
        
        # 学习率调度器
        if args.use_lr_scheduler:
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, 'min', patience=3, factor=0.5
            )

    @staticmethod
    def set_seed(seed):
        torch.manual_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    def get_loss_function(self, loss_name):
        if loss_name == 'quantile':
            return QuantileLoss([0.1, 0.5, 0.9])
        elif loss_name == 'mse':
            return nn.MSELoss()
        elif loss_name == 'mae':
            return nn.L1Loss()
        else:
            raise ValueError(f"Unsupported loss function: {loss_name}")

    def train(self, train_loader):
        self.model.train()
        total_loss = 0.0
        
        for batch in train_loader:
            self.optimizer.zero_grad()
            
            inputs, target = self._prepare_batch(batch)
            
            # 前向传播
            outputs = self.model(**inputs)
            #print(outputs)
            loss = self.criterion(outputs, target)
            
            # 反向传播
            loss.backward()
            #torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item() * target.size(0)
            
        return total_loss / len(train_loader.dataset)

    def validate(self, val_loader):
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            for batch in val_loader:
                # 数据加载与训练相同
                inputs, target = self._prepare_batch(batch)
                outputs = self.model(**inputs)
                loss = self.criterion(outputs, target)
                total_loss += loss.item() * target.size(0)
                
        return total_loss / len(val_loader.dataset)

    def test(self, test_loader):
        self.model.eval()
        if self.static_output_type == 'quantile':
            predictions = {f'q{int(q*100)}': [] for q in [0.1, 0.5, 0.9]}
        else:
            predictions = {'value': []}
        true_values = []
        
        with torch.no_grad():
            for batch in test_loader:
                inputs, target = self._prepare_batch(batch)
                outputs = self.model(**inputs)
                if self.static_output_type == 'quantile':
                    for i, q in enumerate([0.1, 0.5, 0.9]):
                        key = f'q{int(q*100)}'
                        predictions[key].extend(outputs[:,i].cpu().numpy())
                else:
                    predictions['value'].extend(outputs.cpu().numpy().flatten())
                true_values.extend(target.cpu().numpy())
        
        return predictions, np.array(true_values)

    def _prepare_batch(self, batch):
        """ 统一处理批次数据 """
        (input_sequence, target_sequence, target, 
         numeric_features, label_features, 
         temporal_features, activity_text, 
         similar_sequences) = batch
        
        inputs = {
            'input_sequence': input_sequence.to(self.device),
            'numeric_features': numeric_features.to(self.device),
            'label_features': label_features.to(self.device),
            'temporal_features': temporal_features.to(self.device),
            'activity_text': activity_text,
            'similar_sequences': similar_sequences.to(self.device)
        }
        target = target.to(self.device)
        return inputs, target

    def plot_results(self, true_values, predictions, num_samples=500):
        """ 分位数预测或单点预测可视化，风格与dynamic_exp一致 """
        import matplotlib
        plt.rcParams['font.family'] = 'Times New Roman'
        output_dir = os.path.join(
            self.results_dir,
            f"{self.args.model_type}_epoch{self.args.static_epochs:03d}_seed{self.args.random_seed}"
        )
        os.makedirs(output_dir, exist_ok=True)
        true_values = np.asarray(true_values).flatten()
        if self.static_output_type == 'quantile':
            pred_dict = {
                'q10': np.asarray(predictions['q10']).flatten(),
                'q50': np.asarray(predictions['q50']).flatten(),
                'q90': np.asarray(predictions['q90']).flatten()
            }
        else:
            pred_dict = {'value': np.asarray(predictions['value']).flatten()}
        max_samples = len(true_values)
        num_samples = min(num_samples, max_samples) if num_samples else max_samples
        idx = np.random.choice(len(true_values), num_samples, replace=False)
        tv_sample = true_values[idx]
        x_axis = np.arange(num_samples)
        plt.figure(figsize=(10, 6))
        ax = plt.gca()
        ax.set_facecolor('white')
        if self.static_output_type == 'quantile':
            q10_sample = pred_dict['q10'][idx]
            q50_sample = pred_dict['q50'][idx]
            q90_sample = pred_dict['q90'][idx]
            # 置信区间
            plt.fill_between(
                x_axis, 
                q10_sample,
                q90_sample,
                color='#B7E2F0',  # 更淡的蓝色
                alpha=0.4,
                label='80% Confidence Interval'
            )
            plt.plot(x_axis, q10_sample, color='#2C91ED', linewidth=2, linestyle=':', label='10th Percentile')
            plt.plot(x_axis, q50_sample, color='#F0A73A', linewidth=2, label='Median Prediction')
            plt.plot(x_axis, q90_sample, color='#3ABF99', linewidth=2, linestyle='--', label='90th Percentile')
            plt.plot(x_axis, tv_sample, color='#3A3A3A', linewidth=2, linestyle='-', label='True Values')
            coverage = np.mean((tv_sample >= q10_sample) & (tv_sample <= q90_sample))
            mae_q50 = mean_absolute_error(tv_sample, q50_sample)
            interval_width = np.mean(q90_sample - q10_sample)
            stats_text = (
                f'Coverage Rate: {coverage:.1%}\n'
                f'Median MAE: {mae_q50:.2f}\n'
                f'Average Interval Width: {interval_width:.2f}'
            )
            plt.text(0.70, 0.15, stats_text, transform=ax.transAxes,
                    fontsize=12, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            plt.legend(loc='upper right', fontsize=12)
            plt.title('Quantile Regression Prediction Analysis', fontsize=16)
            np.save(os.path.join(output_dir, 'true_values.npy'), true_values)
            for q_key in ['q10', 'q50', 'q90']:
                np.save(os.path.join(output_dir, f'{q_key}_predictions.npy'), pred_dict[q_key])
        else:
            value_sample = pred_dict['value'][idx]
            plt.plot(x_axis, value_sample, color='#F0A73A', linewidth=2, label='Prediction')
            plt.plot(x_axis, tv_sample, color='#3ABF99', linewidth=2, linestyle='-', label='True Values')
            mae = mean_absolute_error(tv_sample, value_sample)
            stats_text = f'MAE: {mae:.2f}'
            plt.text(0.70, 0.15, stats_text, transform=ax.transAxes,
                    fontsize=12, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            plt.legend(loc='upper right', fontsize=12)
            plt.title('Value Prediction Analysis', fontsize=16)
            np.save(os.path.join(output_dir, 'true_values.npy'), true_values)
            np.save(os.path.join(output_dir, 'value_predictions.npy'), pred_dict['value'])
        plt.xlabel('Sample Index', fontsize=14)
        plt.ylabel('Target Value', fontsize=14)
        plt.grid(True, which='both', linestyle='--', linewidth=0.5)
        plt.savefig(os.path.join(output_dir, 'quantile_visualization.png'), dpi=300, bbox_inches='tight')
        plt.close()

    def run(self, train_loader, test_loader):
        self.set_seed(self.args.random_seed)
        best_val_loss = float('inf')
        
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)
        if not os.path.exists(self.model_save_dir):
            os.makedirs(self.model_save_dir)
        num_epochs = self.args.static_epochs
        for epoch in range(num_epochs):
            # 训练阶段
            train_loss = self.train(train_loader)
            
            # 验证阶段
            val_loss = self.validate(test_loader)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), self.model_save_path)
            
            
            # 打印日志
            print(f'Epoch {epoch+1}/{num_epochs}')
            print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')
            # 更新学习率调度器
            if self.args.use_lr_scheduler:
                self.scheduler.step(val_loss)  # 使用 val_loss 作为监控指标
                # self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f'Epoch {epoch+1}, Updated Learning Rate: {current_lr:.6f}')
            print('-' * 50)
        
        # 最终测试
        self.model.load_state_dict(torch.load(self.model_save_path))
        predictions, true_values = self.test(test_loader)
        self.plot_results(true_values, predictions)
        
        # 保存预测结果
        with open(self.results_file, 'a') as f:
            f.write(f'Epochs {num_epochs},  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\n')
        
        return predictions, true_values


===============================================
6. 动态预测实验 - utils/dynamic_exp.py
===============================================
功能：动态预测实验执行，支持时间序列预测


import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from torch.optim.lr_scheduler import ReduceLROnPlateau
from models.WXM import WXM
from models.DDN import DDN
import numpy as np
import os


class DynamicExp:
    def __init__(self, args):
        self.args = args
        self.device = torch.device(f'cuda:{args.gpu_num}' if torch.cuda.is_available() else 'cpu')
        self.results_dir = 'results/dynamic'
        self.results_file = os.path.join(self.results_dir, 'dynamic_results.txt')
        self.model_save_dir = 'models'
        self.model_save_path = os.path.join(self.model_save_dir, f"{args.model_type}_{args.task_type}.pth")

        # 初始化模型
        self.ddn_model = DDN(args).to(self.device) if args.use_ddn_normalization else None
        self.model = WXM(args).to(self.device)

        # 初始化损失函数和优化器
        self.criterion = self.get_loss_function(args.loss)
        self.optimizer = optim.Adam(self.model.parameters(), lr=args.dynamic_learning_rate)
        self.station_optim = optim.Adam(self.ddn_model.parameters(), lr=args.station_lr) if args.use_ddn_normalization else None
        # 学习率调度器
        if args.use_lr_scheduler:
            try:
                self.scheduler = ReduceLROnPlateau(
                    self.optimizer,  # 需要调度的优化器
                    mode='min',      # 监控的指标需要最小化（如 loss）
                    factor=0.5,      # 学习率降低的因子（new_lr = lr * factor）
                    patience=5,      # 等待 5 个 epoch，如果指标没有改善，则降低学习率
                    verbose=True,    # 打印学习率更新的信息
                    threshold=1e-4,  # 指标变化的阈值，只有变化超过阈值才认为是改善
                    threshold_mode='rel',  # 使用相对变化（'rel'）或绝对变化（'abs'）
                    cooldown=0,      # 降低学习率后的冷却时间（epoch 数）
                    min_lr=1e-6      # 学习率的下限
                )
            except TypeError:
                # 如果verbose参数不被支持，使用简化版本
                self.scheduler = ReduceLROnPlateau(
                    self.optimizer,
                    mode='min',
                    factor=0.5,
                    patience=5,
                    threshold=1e-4,
                    threshold_mode='rel',
                    cooldown=0,
                    min_lr=1e-6
                )

    @staticmethod
    def get_loss_function(loss_name):
        if loss_name == 'mse':
            return nn.MSELoss(reduction='mean')
        elif loss_name == 'mae':
            return nn.L1Loss(reduction='mean')
        else:
            raise ValueError(f"Unsupported loss function: {loss_name}")

    def station_loss(self, y, statistics_pred):
        # 输入y 的形状为 [b, pre_len]
        # statistics_pred 的形状为 [b, pre_len, 2],2个维度分别是标准差和均值
        y = y.unsqueeze(-1)
       
        bs, len, dim= y.shape
        # mean [bs,pre_len]
        mean = torch.mean(y, dim=2)
        # 手动计算标准差
        # 将 mean 的形状扩展为与 y 匹配
        mean_expanded = mean.unsqueeze(-1)
        var = torch.mean((y - mean_expanded) ** 2, dim=2)  # 计算方差
        # std [bs,pre_len]
        std = torch.sqrt(var + 1e-7)                       # 计算标准差，添加微小常数
        #求出来的std值为nan
        #std = torch.std(y, dim=2)
        # 将 mean 和 std 拼接为 [b, pre_len, 2]
        station_true = torch.stack([mean, std], dim=-1)  # 使用 stack 而不是 cat
        #station_ture = torch.cat([mean, std], dim=-1)
        #print(station_true.shape)
        # 调整 station_ture 的形状为 [bs, pre_len, 2]
        #station_ture = station_ture.view(bs, self.args.predict_seq_len, 2)
        loss = self.criterion(statistics_pred, station_true)
        return loss
    def train(self, train_loader, epoch, station_pretrain_epoch):
        self.model.train()
        if self.args.use_ddn_normalization:
            self.ddn_model.train()
        train_loss = 0.0

        for i, batch in enumerate(train_loader):
            try:
                self.optimizer.zero_grad()
                if self.args.use_ddn_normalization:
                    self.station_optim.zero_grad()

                input_sequence, target_sequence, target, numeric_features, label_features, temporal_features, activity_text, similar_sequences = batch
                input_sequence = input_sequence.to(self.device)
                target_sequence = target_sequence.to(self.device)
                numeric_features = numeric_features.to(self.device)
                label_features = label_features.to(self.device)
                temporal_features = temporal_features.to(self.device)
                similar_sequences = similar_sequences.to(self.device)

                # 将 input_sequence 从 [batch_size, seq_len] 扩展为 [batch_size, seq_len, 1]
                input_sequence = input_sequence.unsqueeze(-1)

                # DDN 归一化
                if self.args.use_ddn_normalization and epoch + 1 <= station_pretrain_epoch:
                    input_sequence, statistics_pred, _ = self.ddn_model.normalize(input_sequence)
                    loss = self.station_loss(target_sequence, statistics_pred)
                else:
                    if self.args.use_ddn_normalization:
                        input_sequence, statistics_pred, _ = self.ddn_model.normalize(input_sequence)
                        #print("input_sequence shape",input_sequence.shape)
                        output = self.model(input_sequence.squeeze(-1), numeric_features, label_features, temporal_features, activity_text, similar_sequences)
                        #print("output shape",output.shape)
                        output = self.ddn_model.de_normalize(output.unsqueeze(-1), statistics_pred)
                        #print("output shape",output.shape)
                    else:
                        #模型output变成nan了
                        output = self.model(input_sequence.squeeze(-1), numeric_features, label_features, temporal_features, activity_text, similar_sequences)
                    # output[B, seq_len, 1] -> [B, seq_len]
                    loss = self.criterion(output.squeeze(-1), target_sequence)

                # 反向传播和参数更新
                loss.backward()
                if self.args.use_ddn_normalization and epoch + 1 <= station_pretrain_epoch:
                    self.station_optim.step()
                else:
                    self.optimizer.step()
                    if self.args.use_ddn_normalization:
                        self.station_optim.step()

                train_loss += loss.item() * target_sequence.size(0)

            except Exception as e:
                print(f"Error in batch {i}: {e}")
                raise

        return train_loss / len(train_loader.dataset)

    def validate(self, val_loader, epoch, station_pretrain_epoch):
        self.model.eval()
        if self.args.use_ddn_normalization:
            self.ddn_model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                input_sequence, target_sequence, target, numeric_features, label_features, temporal_features, activity_text, similar_sequences = batch
                input_sequence = input_sequence.to(self.device)
                target_sequence = target_sequence.to(self.device)
                numeric_features = numeric_features.to(self.device)
                label_features = label_features.to(self.device)
                temporal_features = temporal_features.to(self.device)
                similar_sequences = similar_sequences.to(self.device)

                input_sequence = input_sequence.unsqueeze(-1)

                if self.args.use_ddn_normalization and epoch + 1 <= station_pretrain_epoch:
                    input_sequence, statistics_pred, _ = self.ddn_model.normalize(input_sequence, p_value=False)
                    loss = self.station_loss(target_sequence, statistics_pred)
                else:
                    if self.args.use_ddn_normalization:
                        input_sequence, statistics_pred, _ = self.ddn_model.normalize(input_sequence)
                        output = self.model(input_sequence.squeeze(-1), numeric_features, label_features, temporal_features, activity_text, similar_sequences)
                        output = self.ddn_model.de_normalize(output.unsqueeze(-1), statistics_pred)
                    else:
                        output = self.model(input_sequence.squeeze(-1), numeric_features, label_features, temporal_features, activity_text, similar_sequences)
                    loss = self.criterion(output.squeeze(-1), target_sequence)

                val_loss += loss.item() * target_sequence.size(0)

        torch.cuda.empty_cache()
        return val_loss / len(val_loader.dataset)

    def test(self, test_loader):
        self.model.eval()
        if self.args.use_ddn_normalization:
            self.ddn_model.eval()
        predictions = []
        true_values = []
        current_values_list = []

        with torch.no_grad():
            for batch in test_loader:
                input_sequence, target_sequence, target, numeric_features, label_features, temporal_features, activity_text, similar_sequences = batch
                input_sequence = input_sequence.to(self.device)
                target_sequence = target_sequence.to(self.device)
                numeric_features = numeric_features.to(self.device)
                label_features = label_features.to(self.device)
                temporal_features = temporal_features.to(self.device)
                similar_sequences = similar_sequences.to(self.device)

                input_sequence = input_sequence.unsqueeze(-1)
                #注意：需要在ddn归一化前保存输入序列真实值，不然可能会出现负数
                current_values_list.append(input_sequence.cpu().numpy())
                if self.args.use_ddn_normalization:
                    input_sequence, statistics_pred, _ = self.ddn_model.normalize(input_sequence)
                    output = self.model(input_sequence.squeeze(-1), numeric_features, label_features, temporal_features, activity_text, similar_sequences)
                    output = self.ddn_model.de_normalize(output.unsqueeze(-1), statistics_pred)
                else:
                    output = self.model(input_sequence.squeeze(-1), numeric_features, label_features, temporal_features, activity_text, similar_sequences)
                output = output.squeeze(-1)

                predictions.append(output.cpu().numpy())
                true_values.append(target_sequence.cpu().numpy())

        # Predictions shape: (123, 7)
        # True values shape: (123, 7)
        # Current values shape: (123, 14, 1)
        predictions = np.concatenate(predictions, axis=0)
        true_values = np.concatenate(true_values, axis=0)
        current_values = np.concatenate(current_values_list, axis=0)
        print(f"Predictions shape: {predictions.shape}")
        print(f"True values shape: {true_values.shape}")
        print(f"Current values shape: {current_values.shape}")
        return predictions, true_values, current_values

    def plot_results(self, true_values, predictions, current_values):
        plt.rcParams['font.family'] = 'Times New Roman'

        filename = f"{self.args.model_type}_epoch{self.args.dynamic_epochs}_{self.args.current_seq_len}_{self.args.predict_seq_len}_{self.args.total_seq_len}_{self.args.use_ddn_normalization}_seed{self.args.random_seed}"
        output_dir = os.path.join(self.results_dir, filename)
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        for i in range(len(current_values)):
            plt.figure(figsize=(10, 6))
            con_true_values = np.concatenate([current_values[i].flatten(), true_values[i].flatten()])
            con_predictions = np.concatenate([current_values[i].flatten(), predictions[i].flatten()])
            plt.plot(range(self.args.current_seq_len + self.args.predict_seq_len), con_true_values, label='Ground Truth', color='#3ABF99', linewidth=2)
            plt.plot(range(self.args.current_seq_len + self.args.predict_seq_len), con_predictions, label='Prediction', color='#F0A73A', linewidth=2)
            plt.plot(range(self.args.current_seq_len), current_values[i].flatten(), label='Input Data', color='#2C91ED', linewidth=2)
            plt.legend(loc="upper right")
            plt.xlabel('Time Step', fontsize=14)
            plt.ylabel('OC', fontsize=14)
            plt.title('Predictions vs True Values', fontsize=16)
            plt.legend(fontsize=12)
            plt.gca().set_facecolor('white')
            plt.grid(True, which='both', linestyle='--', linewidth=0.5)
            plt.savefig(os.path.join(output_dir, f"{i+1}.png"))
            plt.close()

        
        np.save(os.path.join(output_dir,"_predictions.npy"), predictions)
        np.save(os.path.join(output_dir,"_true_values.npy"), true_values)

    def run(self, train_loader, test_loader):
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)
        if not os.path.exists(self.model_save_dir):
            os.makedirs(self.model_save_dir)

        num_epochs = self.args.dynamic_epochs
        station_pretrain_epoch = self.args.pre_epoch

        for epoch in range(num_epochs + station_pretrain_epoch):
            train_loss = self.train(train_loader, epoch, station_pretrain_epoch)
            val_loss = self.validate(test_loader, epoch, station_pretrain_epoch)
            if epoch + 1 <= station_pretrain_epoch:
                print(f'Station Pretrain Epoch {epoch+1}/{station_pretrain_epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
            else:
                print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
            # 更新学习率调度器
            if self.args.use_lr_scheduler:
                self.scheduler.step(val_loss)  # 使用 val_loss 作为监控指标
                # self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f'Epoch {epoch+1}, Updated Learning Rate: {current_lr:.6f}')
            print('-' * 50)

        predictions, true_values, current_values = self.test(test_loader)
        mae = mean_absolute_error(true_values, predictions)
        print(f'Mean Absolute Error: {mae:.4f}')
        mse = mean_squared_error(true_values, predictions)
        print(f'Mean Squared Error: {mse:.4f}')
        mape = mean_absolute_percentage_error(true_values, predictions)
        print(f'Mean Absolute Percentage Error: {mape:.4f}')
        self.plot_results(true_values, predictions, current_values)
        
        with open(self.results_file, 'a') as f:
            f.write(f'Epochs {num_epochs}, MAE: {mae:.4f}, MSE:{mse:.4f}, MAPE:{mape:.4f}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f},current_seq_len: {self.args.current_seq_len},predict_seq_len: {self.args.predict_seq_len},total_seq_len: {self.args.total_seq_len}\n')

        # 保存WXM模型
        torch.save(self.model.state_dict(), self.model_save_path)
        print(f'WXM Model saved to {self.model_save_path}')
        
        # 如果使用DDN，也保存DDN模型参数
        if self.args.use_ddn_normalization and self.ddn_model is not None:
            ddn_save_path = os.path.join(self.model_save_dir, f"DDN_{self.args.task_type}.pth")
            torch.save(self.ddn_model.state_dict(), ddn_save_path)
            print(f'DDN Model saved to {ddn_save_path}')
        
        return predictions, true_values


===============================================
7. RevIN可逆归一化层 - layers/RevIN.py
===============================================
功能：可逆归一化层，用于时间序列数据的归一化和反归一化


import torch
import torch.nn as nn

class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):
        """
        :param num_features: the number of features or channels
        :param eps: a value added for numerical stability
        :param affine: if True, RevIN has learnable affine parameters
        """
        super(RevIN, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        self.subtract_last = subtract_last
        if self.affine:
            self._init_params()

    def forward(self, x, mode:str):
        if mode == 'norm':
            self._get_statistics(x)
            x = self._normalize(x)
        elif mode == 'denorm':
            x = self._denormalize(x)
        else: raise NotImplementedError
        return x

    def _init_params(self):
        # initialize RevIN params: (C,)
        self.affine_weight = nn.Parameter(torch.ones(self.num_features))
        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

    def _get_statistics(self, x):
        dim2reduce = tuple(range(1, x.ndim-1))
        if self.subtract_last:
            self.last = x[:,-1,:].unsqueeze(1)
        else:
            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

    def _normalize(self, x):
        if self.subtract_last:
            x = x - self.last
        else:
            x = x - self.mean
        x = x / self.stdev
        if self.affine:
            x = x * self.affine_weight
            x = x + self.affine_bias
        return x

    def _denormalize(self, x):
        if self.affine:
            x = x - self.affine_bias
            x = x / (self.affine_weight + self.eps*self.eps)
        x = x * self.stdev
        if self.subtract_last:
            x = x + self.last
        else:
            x = x + self.mean
        return x


===============================================
8. 特征分布标准化层 - layers/fds.py
===============================================
功能：特征分布标准化，用于改善模型训练稳定性


import logging
import numpy as np
from scipy.ndimage import gaussian_filter1d
from scipy.signal.windows import triang
import torch
import torch.nn as nn
import torch.nn.functional as F


print = logging.info

def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):
    if torch.sum(v1) < 1e-10:
        return matrix
    if (v1 == 0.).any():
        valid = (v1 != 0.)
        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)
        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]
        return matrix

    factor = torch.clamp(v2 / v1, clip_min, clip_max)
    return (matrix - m1) * torch.sqrt(factor) + m2

class FDS(nn.Module):

    def __init__(self, feature_dim, bucket_num=100, bucket_start=3, start_update=0, start_smooth=1,
                 kernel='gaussian', ks=5, sigma=2, momentum=0.9):
        super(FDS, self).__init__()
        self.feature_dim = feature_dim
        self.bucket_num = bucket_num
        self.bucket_start = bucket_start
        self.kernel_window = self._get_kernel_window(kernel, ks, sigma)
        self.half_ks = (ks - 1) // 2
        self.momentum = momentum
        self.start_update = start_update
        self.start_smooth = start_smooth

        self.register_buffer('epoch', torch.zeros(1).fill_(start_update))
        self.register_buffer('running_mean', torch.zeros(bucket_num - bucket_start, feature_dim))
        self.register_buffer('running_var', torch.ones(bucket_num - bucket_start, feature_dim))
        self.register_buffer('running_mean_last_epoch', torch.zeros(bucket_num - bucket_start, feature_dim))
        self.register_buffer('running_var_last_epoch', torch.ones(bucket_num - bucket_start, feature_dim))
        self.register_buffer('smoothed_mean_last_epoch', torch.zeros(bucket_num - bucket_start, feature_dim))
        self.register_buffer('smoothed_var_last_epoch', torch.ones(bucket_num - bucket_start, feature_dim))
        self.register_buffer('num_samples_tracked', torch.zeros(bucket_num - bucket_start))

    @staticmethod
    def _get_kernel_window(kernel, ks, sigma):
        assert kernel in ['gaussian', 'triang', 'laplace']
        half_ks = (ks - 1) // 2
        if kernel == 'gaussian':
            base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks
            base_kernel = np.array(base_kernel, dtype=np.float32)
            kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / sum(gaussian_filter1d(base_kernel, sigma=sigma))
        elif kernel == 'triang':
            kernel_window = triang(ks) / sum(triang(ks))
        else:
            laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)
            kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / sum(map(laplace, np.arange(-half_ks, half_ks + 1)))

        print(f'Using FDS: [{kernel.upper()}] ({ks}/{sigma})')
        return torch.tensor(kernel_window, dtype=torch.float32).cuda()

    def _update_last_epoch_stats(self):
        self.running_mean_last_epoch = self.running_mean
        self.running_var_last_epoch = self.running_var

        # 确保输入张量和权重张量在同一个设备上
        kernel_window = self.kernel_window.to(self.running_mean_last_epoch.device)

        self.smoothed_mean_last_epoch = F.conv1d(
            input=F.pad(self.running_mean_last_epoch.unsqueeze(1).permute(2, 1, 0),
                        pad=(self.half_ks, self.half_ks), mode='reflect'),
            weight=kernel_window.view(1, 1, -1), padding=0
        ).permute(2, 1, 0).squeeze(1)
        self.smoothed_var_last_epoch = F.conv1d(
            input=F.pad(self.running_var_last_epoch.unsqueeze(1).permute(2, 1, 0),
                        pad=(self.half_ks, self.half_ks), mode='reflect'),
            weight=kernel_window.view(1, 1, -1), padding=0
        ).permute(2, 1, 0).squeeze(1)

    def reset(self):
        self.running_mean.zero_()
        self.running_var.fill_(1)
        self.running_mean_last_epoch.zero_()
        self.running_var_last_epoch.fill_(1)
        self.smoothed_mean_last_epoch.zero_()
        self.smoothed_var_last_epoch.fill_(1)
        self.num_samples_tracked.zero_()

    def update_last_epoch_stats(self, epoch):
        if epoch == self.epoch + 1:
            self.epoch += 1
            self._update_last_epoch_stats()
            print(f"Updated smoothed statistics on Epoch [{epoch}]!")

    def update_running_stats(self, features, labels, epoch):
        if epoch < self.epoch:
            return

        assert self.feature_dim == features.size(1), "Input feature dimension is not aligned!"
        assert features.size(0) == labels.size(0), "Dimensions of features and labels are not aligned!"

        for label in torch.unique(labels):
            if label > self.bucket_num - 1 or label < self.bucket_start:
                continue
            elif label == self.bucket_start:
                curr_feats = features[labels <= label]
            elif label == self.bucket_num - 1:
                curr_feats = features[labels >= label]
            else:
                curr_feats = features[labels == label]
            curr_num_sample = curr_feats.size(0)
            curr_mean = torch.mean(curr_feats, 0)
            curr_var = torch.var(curr_feats, 0, unbiased=True if curr_feats.size(0) != 1 else False)

            self.num_samples_tracked[int(label - self.bucket_start)] += curr_num_sample
            factor = self.momentum if self.momentum is not None else \
                (1 - curr_num_sample / float(self.num_samples_tracked[int(label - self.bucket_start)]))
            factor = 0 if epoch == self.start_update else factor
            self.running_mean[int(label - self.bucket_start)] = \
                (1 - factor) * curr_mean + factor * self.running_mean[int(label - self.bucket_start)]
            self.running_var[int(label - self.bucket_start)] = \
                (1 - factor) * curr_var + factor * self.running_var[int(label - self.bucket_start)]

        print(f"Updated running statistics with Epoch [{epoch}] features!")

    def smooth(self, features, labels, epoch):
        if epoch < self.start_smooth:
            return features
        # labels = labels.squeeze(1)
        for label in torch.unique(labels):
            if label > self.bucket_num - 1 or label < self.bucket_start:
                continue
            elif label == self.bucket_start:
                features[labels <= label] = calibrate_mean_var(
                    features[labels <= label],
                    self.running_mean_last_epoch[int(label - self.bucket_start)],
                    self.running_var_last_epoch[int(label - self.bucket_start)],
                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],
                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])
            elif label == self.bucket_num - 1:
                features[labels >= label] = calibrate_mean_var(
                    features[labels >= label],
                    self.running_mean_last_epoch[int(label - self.bucket_start)],
                    self.running_var_last_epoch[int(label - self.bucket_start)],
                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],
                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])
            else:
                features[labels == label] = calibrate_mean_var(
                    features[labels == label],
                    self.running_mean_last_epoch[int(label - self.bucket_start)],
                    self.running_var_last_epoch[int(label - self.bucket_start)],
                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],
                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])
        return features



===============================================
9. 工具函数 - utils/tools.py
===============================================
功能：通用工具函数，数据预处理，评估指标计算


import numpy as np
import torch
import matplotlib.pyplot as plt
import time
plt.grid

plt.switch_backend('agg')


def adjust_learning_rate(optimizer, epoch, args, learning_rate):
    # lr = args.learning_rate * (0.2 ** (epoch // 2))
    if args.lradj == 'type1':
        lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}
    elif args.lradj == 'type2':
        lr_adjust = {
            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,
            10: 5e-7, 15: 1e-7, 20: 5e-8
        }
    elif args.lradj == 'type3':
        lr_adjust = {}
    elif args.lradj == '3':
        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}
    elif args.lradj == '4':
        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}
    elif args.lradj == '5':
        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}
    elif args.lradj == '6':
        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}  
    if epoch in lr_adjust.keys():
        lr = lr_adjust[epoch]
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        print('Updating learning rate to {}'.format(lr))


class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta

    def __call__(self, val_loss, model, path, station_model=None, station_path=None):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path, station_model, station_path)
        elif score < self.best_score + self.delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path, station_model, station_path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, path, station_model=None, station_path=None):
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')
        if station_model is not None and station_path is not None:
            print('station model has replaced')
            torch.save(station_model.state_dict(), station_path + '/' + 'checkpoint.pth')
        self.val_loss_min = val_loss


class dotdict(dict):
    """dot.notation access to dictionary attributes"""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


class StandardScaler():
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def transform(self, data):
        return (data - self.mean) / self.std

    def inverse_transform(self, data):
        return (data * self.std) + self.mean


def visual(true, preds=None, name='./pic/test.pdf'):
    """
    Results visualization
    """
    plt.figure()
    plt.plot(true, label='GroundTruth', linewidth=2)
    if preds is not None:
        plt.plot(preds, label='Prediction', linewidth=2)
    plt.legend()
    plt.savefig(name, bbox_inches='tight')

def test_params_flop(model,x_shape):
    """
    If you want to thest former's flop, you need to give default value to inputs in model.forward(), the following code can only pass one argument to forward()
    """
    model_params = 0
    for parameter in model.parameters():
        model_params += parameter.numel()
        print('INFO: Trainable parameter count: {:.2f}M'.format(model_params / 1000000.0))
    from ptflops import get_model_complexity_info    
    with torch.cuda.device(0):
        macs, params = get_model_complexity_info(model.cuda(), x_shape, as_strings=True, print_per_layer_stat=True)
        # print('Flops:' + flops)
        # print('Params:' + params)
        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))
        print('{:<30}  {:<8}'.format('Number of parameters: ', params))


===============================================
10. 配置文件 - configs/config.yaml
===============================================
功能：系统配置文件，包含所有训练参数和模型配置


# =============================================================================
# WXM多模态预测模型配置文件
# 该配置文件定义了模型训练和预测的所有关键参数
# =============================================================================

# =============================================================================
# 系统环境配置 (System Environment Configuration)（不用调）
# =============================================================================
random_seed: 2024          # 随机种子，确保实验可重复性
gpu_num: 0                 # GPU设备编号，0表示第一个GPU，-1表示使用CPU
log_dir: "log"             # 日志文件保存目录

# =============================================================================
# 数据路径配置 (Data Path Configuration)（不用调）
# =============================================================================
data:
  original_data_path: "data/activity_order_independent_id"    # 原始活动订单数据路径
  historical_activities_path: "data"                          # 历史活动数据路径
  activities_attributes_path: "data"                          # 活动属性数据路径
  dataset_df_path: "data/dataset_df.csv"                      # 处理后的数据集文件路径

# =============================================================================
# 预测目标配置 (Prediction Target Configuration)（可随意调节）
# =============================================================================
prediction:
  freq: "d"                                                   # 数据频率：h(小时), d(天), w(周), m(月)
  target: "uc"                                                # 预测目标：oc(订单量), uc(用户量)
  static_target: "user_sum"                                   # 静态预测目标：oc_sum(订单总数), user_sum(用户总数)

# =============================================================================
# 模型架构配置 (Model Architecture Configuration)（可随意调节）
# =============================================================================
model:
  model_type: "WXM"                    # 模型类型：WXM(多模态融合模型)
  embedding_dim: 32                     # 特征嵌入维度，用于文本和标签特征编码（需要注意为4的整数倍）
  d_model: 128                          # 模型主隐藏层维度，Transformer架构的核心维度（需要注意为4的整数倍）
  d_s: 64                               # 静态预测分支隐藏层维度，用于处理活动属性特征（需要注意为4的整数倍）
  d_n: 64                               # 动态预测分支隐藏层维度，用于处理时间序列特征（需要注意为4的整数倍）
  num_attn_heads: 1                     # 多头注意力机制中的头数，影响特征交互能力（最好不要超过8）
  num_hidden_layers: 2                  # Transformer编码器的隐藏层数量（最好不要超过6层）
  dropout: 0.4                          # Dropout比率，防止过拟合（最好不要超过0.5）
  use_encoder_mask: 1                   # 是否使用编码器掩码，1启用，0禁用
  autoregressive: 0                     # 是否使用自回归模式，0禁用，1启用（如果为1，则需要将use_encoder_mask设置为0）

# =============================================================================
# 特征工程配置 (Feature Engineering Configuration)（可随意调节）
# =============================================================================
features:
  use_img: false                        # 是否使用图像特征，false表示不使用
  use_text_attributes: true             # 是否使用文本属性特征（活动名称、标题、产品名称）
  use_label_attributes: true            # 是否使用标签属性特征（客户ID、模板ID、活动类型等）
  use_numeric_attributes: true          # 是否使用数值属性特征（预算、奖励数量、持续时间）
  use_temporal_features: true           # 是否使用时间特征（日、周、月、年）
  use_current_seq: true                 # 是否使用当前序列特征
  use_similar: false                    # 是否使用相似序列特征

# =============================================================================
# 训练超参数配置 (Training Hyperparameters Configuration)（可随意调节）
# =============================================================================
training:
  batch_size: 8                         # 训练批次大小，影响内存使用和训练稳定性（显卡内存大可以调大，否则调小，需要为4的整数倍）
  use_lr_scheduler: true                # 是否使用学习率调度器，动态调整学习率
  loss: "mae"                           # 损失函数类型：mae(平均绝对误差), mse(均方误差), mape(平均绝对百分比误差)

# =============================================================================
# 静态预测训练配置 (Static Prediction Training Configuration)（可随意调节）
# =============================================================================
static_training:
  static_epochs: 100                     # 静态预测模型训练轮数
  static_learning_rate: 0.1             # 静态预测模型学习率（不要太大<=0.1，否则容易过拟合）
  static_output_type: "value"           # 静态预测输出类型：value(单点预测), quantile(分位数预测)
  num_gated_blocks: 2                   # 门控残差块数量，用于特征权重分配和梯度优化

# =============================================================================
# 动态预测训练配置 (Dynamic Prediction Training Configuration)（可随意调节）
# =============================================================================
dynamic_training:
  dynamic_epochs: 100                   # 动态预测模型训练轮数
  dynamic_learning_rate: 0.1            # 动态预测模型学习率（不要太大<=0.1，否则容易过拟合）

# =============================================================================
# 序列处理配置 (Sequence Processing Configuration)（仅可调节前俩项参数）
# =============================================================================
sequence:
  current_seq_len: 14                   # 当前序列长度，输入的历史数据天数
  predict_seq_len: 7                    # 预测序列长度，输出的预测数据天数
  total_seq_len: 30                     # 总序列长度，包括历史、当前和预测的总天数
  sequence_process_type: "LTTB"         # 序列处理类型：LTTB(下采样), autoencoder(自编码器)
  apply_diff: true                      # 是否对序列数据应用差分处理，减少趋势影响
  apply_smoothing: true                 # 是否对序列数据应用平滑处理，减少噪声影响
  series_scale: false                   # 是否对序列数据进行缩放，false表示不缩放
  feature_scale: true                   # 是否对特征数据进行缩放，true表示启用特征标准化

# =============================================================================
# DDN归一化配置 (DDN Normalization Configuration)（不用调）
# DDN: Denoising Diffusion Normalization，去噪扩散归一化
# =============================================================================
ddn:
  use_ddn_normalization: true           # 是否启用DDN归一化，自适应时间序列归一化
  j: 0                                  # 小波分解层数，0表示不进行小波分解
  learnable: false                      # 是否使用可学习的DDN参数，false表示使用固定参数
  wavelet: "coif3"                      # 小波基函数类型：coif3(Coiflet小波)，用于信号分解
  dr: 0.01                              # 扩散率，控制归一化过程的强度
  pre_epoch: 5                          # 预训练轮数，DDN模型预训练阶段
  twice_epoch: 1                        # 二次训练轮数，DDN模型微调阶段
  use_norm: "sliding"                   # 归一化方式：sliding(滑动窗口), global(全局)
  kernel_len: 7                         # 卷积核长度，用于时间序列特征提取
  hkernel_len: 5                        # 水平卷积核长度，用于空间特征提取
  station_lr: 0.0001                    # 平稳化学习率，用于DDN参数优化
  station_type: "adaptive"              # 平稳化类型：adaptive(自适应), fixed(固定)
  pd_ff: 1024                           # 前馈网络隐藏层维度，用于DDN特征变换
  pd_model: 512                         # DDN模型隐藏层维度
  pe_layers: 2                          # 位置编码层数，用于序列位置信息编码 


===============================================
11. 原始数据预处理工具
11.1 活动属性表生成工具 - excel_handle/活动属性处理/活动属性表生成工具.py
功能：从多个CSV文件中提取活动信息，生成包含产品信息的活动属性表
===============================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
活动属性表生成工具
功能：从多个CSV文件中提取活动信息，生成包含产品信息的活动属性表

作者：AI助手
日期：2024年
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
import argparse
import csv

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('activity_attributes_generator.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ActivityAttributesGenerator:
    """活动属性表生成器"""
    
    def __init__(self, input_dir='data/output20250706', output_dir='data', simplified_dir='data'):
        """
        初始化生成器
        
        Args:
            input_dir (str): 输入数据目录
            output_dir (str): 输出目录（最终活动属性表）
            simplified_dir (str): 简化数据输出目录
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.simplified_dir = Path(simplified_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.simplified_dir.mkdir(exist_ok=True)
        
        # 定义需要删除的列
        self.columns_to_drop = [
            'activity_id', 'create_date', 'page_url', 'activity_onoff',
            'activity_status', 'activity_order_overdue', 'del_flg', 'create_user',
            'create_ip', 'update_date', 'update_user', 'update_ip',
            'activity_pay_onoff', 'qe_code_url', 'bd_update_date', 
            'tenant_id', 'te_template_id', 'activity_version'
        ]
        
    def load_data(self):
        """加载所有数据文件"""
        try:
            logger.info("开始加载数据文件...")
            
            # 读取活动订单数据
            activity_order_path = self.input_dir / 'activity_order.csv'
            logger.info(f"正在读取活动订单数据: {activity_order_path}")
            self.activity_order = pd.read_csv(
                activity_order_path, 
                encoding='utf-8',
                low_memory=False
            )
            logger.info(f"活动订单数据加载完成，形状: {self.activity_order.shape}")
            
            # 读取所有活动数据
            all_activity_path = self.input_dir / 'all_activity.csv'
            logger.info(f"正在读取所有活动数据: {all_activity_path}")
            self.all_activity = pd.read_csv(
                all_activity_path,
                encoding='utf-8',
                low_memory=False
            )
            logger.info(f"所有活动数据加载完成，形状: {self.all_activity.shape}")
            
            # 读取订单子表数据
            order_sub_path = self.input_dir / 'order_sub.csv'
            logger.info(f"正在读取订单子表数据: {order_sub_path}")
            self.order_sub = pd.read_csv(
                order_sub_path,
                encoding='utf-8',
                on_bad_lines='skip',
                low_memory=False
            )
            logger.info(f"订单子表数据加载完成，形状: {self.order_sub.shape}")
            
            # 读取商品数据
            goods_path = self.input_dir / 'goods.csv'
            logger.info(f"正在读取商品数据: {goods_path}")
            self.goods = pd.read_csv(
                goods_path,
                encoding='utf-8',
                on_bad_lines='skip',
                low_memory=False
            )
            logger.info(f"商品数据加载完成，形状: {self.goods.shape}")
            
            # 读取产品数据
            product_path = self.input_dir / 'product.csv'
            logger.info(f"正在读取产品数据: {product_path}")
            self.product = pd.read_csv(
                product_path,
                encoding='utf-8',
                on_bad_lines='skip',
                low_memory=False
            )
            logger.info(f"产品数据加载完成，形状: {self.product.shape}")
            
        except Exception as e:
            logger.error(f"数据加载失败: {e}")
            raise
    
    def process_goods_data(self):
        """处理商品数据，聚合产品ID"""
        try:
            logger.info("正在处理商品数据...")
            
            # 对每个activity_id聚合product_id
            goods_grouped = self.goods.groupby('activity_id')['product_id'].apply(list).reset_index()
            logger.info(f"商品数据聚合完成，共 {len(goods_grouped)} 个活动")
            
            return goods_grouped
            
        except Exception as e:
            logger.error(f"商品数据处理失败: {e}")
            raise
    
    def merge_activity_product_data(self, goods_grouped):
        """合并活动数据和产品数据"""
        try:
            logger.info("正在合并活动数据和产品数据...")
            
            # 创建最终结果DataFrame
            final_df = self.all_activity.copy()
            
            # 遍历goods_grouped，为每个activity_id添加product_id列
            for index, row in goods_grouped.iterrows():
                activity_id = row['activity_id']
                product_ids = row['product_id']
                
                # 将product_ids列表转换为字符串
                product_ids_str = ','.join(map(str, product_ids))
                
                # 检查是否需要新增列
                if 'product_ids' not in final_df.columns:
                    final_df['product_ids'] = None
                
                # 在匹配的行中插入product_ids字符串
                final_df.loc[final_df['id'] == activity_id, 'product_ids'] = product_ids_str
            
            logger.info(f"活动数据和产品数据合并完成，形状: {final_df.shape}")
            return final_df
            
        except Exception as e:
            logger.error(f"数据合并失败: {e}")
            raise
    
    def add_product_names_and_types(self, final_df):
        """添加产品名称和类型信息"""
        try:
            logger.info("正在添加产品名称和类型信息...")
            
            # 确保final_df中有存放合并后的product_name和product_type1的列
            if 'product_names' not in final_df.columns:
                final_df['product_names'] = ''
            if 'product_types' not in final_df.columns:
                final_df['product_types'] = ''
            
            # 遍历final_df的每一行
            total_rows = len(final_df)
            for index, row in final_df.iterrows():
                if (index + 1) % 100 == 0:
                    logger.info(f"处理进度: {index + 1}/{total_rows}")
                
                product_ids_str = row['product_ids']  # 获取product_ids字符串
                product_ids = product_ids_str.split(',') if product_ids_str else []
                
                # 初始化空列表来存储product_name和product_type1
                product_names = []
                product_types = []
                
                # 对于每个product_id，从product表中查询对应的product_name和product_type1
                for product_id in product_ids:
                    product_info = self.product[self.product['id'] == product_id]
                    if not product_info.empty:
                        product_names.append(str(product_info['product_name'].iloc[0]))
                        product_types.append(str(product_info['product_type1'].iloc[0]))
                    else:
                        # 改为添加空字符串
                        product_names.append('')
                        product_types.append('')
                
                # 将product_names和product_types列表中的元素转换为字符串，并更新到final_df中
                final_df.at[index, 'product_names'] = ','.join(filter(None, product_names))
                final_df.at[index, 'product_types'] = ','.join(filter(None, product_types))
            
            logger.info("产品名称和类型信息添加完成")
            return final_df
            
        except Exception as e:
            logger.error(f"添加产品信息失败: {e}")
            raise
    
    def clean_data(self, final_df):
        """清理数据，替换空值"""
        try:
            logger.info("正在清理数据...")
            
            # 替换None和空字符串为NaN
            final_df = final_df.replace([None, ''], np.nan)
            
            logger.info("数据清理完成")
            return final_df
            
        except Exception as e:
            logger.error(f"数据清理失败: {e}")
            raise
    
    def add_time_attributes(self, final_df):
        """添加时间相关属性"""
        try:
            logger.info("正在添加时间相关属性...")
            
            # 确保时间列是日期时间格式
            final_df['activity_start_time'] = pd.to_datetime(final_df['activity_start_time'])
            final_df['activity_end_time'] = pd.to_datetime(final_df['activity_end_time'])
            
            # 添加day列
            final_df['day'] = final_df['activity_start_time'].dt.day
            
            # 添加week列
            final_df['week'] = final_df['activity_start_time'].dt.weekday + 1
            
            # 添加month列
            final_df['month'] = final_df['activity_start_time'].dt.month
            
            # 添加year列
            final_df['year'] = final_df['activity_start_time'].dt.year
            
            # 计算持续时间并转换为天数
            final_df['duration_days'] = (final_df['activity_end_time'] - final_df['activity_start_time']).dt.days
            
            logger.info("时间相关属性添加完成")
            return final_df
            
        except Exception as e:
            logger.error(f"添加时间属性失败: {e}")
            raise
    
    def drop_unnecessary_columns(self, final_df):
        """删除不必要的列"""
        try:
            logger.info("正在删除不必要的列...")
            
            # 只删除存在的列
            existing_columns = [col for col in self.columns_to_drop if col in final_df.columns]
            final_df = final_df.drop(columns=existing_columns)
            
            logger.info(f"列删除完成，最终形状: {final_df.shape}")
            logger.info(f"保留的列: {list(final_df.columns)}")
            
            return final_df
            
        except Exception as e:
            logger.error(f"删除列失败: {e}")
            raise
    
    def save_results(self, final_df):
        """保存结果"""
        try:
            logger.info("正在保存结果...")
            
            # 保存补充后的活动数据（简化数据）
            complemented_path = self.simplified_dir / 'all_activity_complemented.csv'
            final_df.to_csv(complemented_path, index=False, encoding='utf-8')
            logger.info(f"补充后的活动数据已保存: {complemented_path}")
            
            # 保存最终的活动属性表
            attributes_path = self.output_dir / 'all_activities_attributes.csv'
            final_df.to_csv(attributes_path, index=False, encoding='utf-8')
            logger.info(f"活动属性表已保存: {attributes_path}")
            
            return complemented_path, attributes_path
            
        except Exception as e:
            logger.error(f"保存结果失败: {e}")
            raise
    
    def run(self):
        """运行完整的生成流程"""
        try:
            logger.info("=== 开始活动属性表生成流程 ===")
            
            # 1. 加载数据
            self.load_data()
            
            # 2. 处理商品数据
            goods_grouped = self.process_goods_data()
            
            # 3. 合并活动数据和产品数据
            final_df = self.merge_activity_product_data(goods_grouped)
            
            # 4. 添加产品名称和类型信息
            final_df = self.add_product_names_and_types(final_df)
            
            # 5. 清理数据
            final_df = self.clean_data(final_df)
            
            # 6. 添加时间相关属性
            final_df = self.add_time_attributes(final_df)
            
            # 7. 删除不必要的列
            final_df = self.drop_unnecessary_columns(final_df)
            
            # 8. 保存结果
            complemented_path, attributes_path = self.save_results(final_df)
            
            logger.info("=== 活动属性表生成流程完成 ===")
            logger.info(f"输出目录: {self.output_dir}")
            logger.info(f"补充数据文件: {complemented_path}")
            logger.info(f"属性表文件: {attributes_path}")
            logger.info(f"最终数据形状: {final_df.shape}")
            
            return final_df
            
        except Exception as e:
            logger.error(f"流程执行失败: {e}")
            raise

def main():
    """主函数"""
    parser = argparse.ArgumentParser(description='活动属性表生成工具')
    parser.add_argument('--input-dir', default='data/output20250706', help='输入数据目录')
    parser.add_argument('--output-dir', default='data', help='输出目录')
    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], help='日志级别')
    
    args = parser.parse_args()
    
    # 设置日志级别
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # 创建生成器并运行
    generator = ActivityAttributesGenerator(args.input_dir, args.output_dir, 'data')
    generator.run()
===============================================
11.2 活动订单数据分割工具 - excel_handle/活动序列处理/活动订单数据分割工具.py
功能：从CSV文件中分割出每个活动的订单数据，并输出为单独的CSV文件
===============================================
if __name__ == "__main__":
    main() #!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
活动订单数据分割工具
功能：从CSV文件中分割出每个活动的订单数据，并输出为单独的CSV文件

日期：2025年7月28日
"""

import pandas as pd
import os
import glob
import logging
from pathlib import Path
import argparse

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('activity_order_splitter.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ActivityOrderSplitter:
    """活动订单数据分割器"""
    
    def __init__(self, input_dir='data/output20250706', output_dir='data/activity_order_independent', simplified_dir='data'):
        """
        初始化分割器
        
        Args:
            input_dir (str): 输入数据目录
            output_dir (str): 输出目录（活动订单分割文件）
            simplified_dir (str): 简化数据输出目录
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.simplified_dir = Path(simplified_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.simplified_dir.mkdir(exist_ok=True)
        
        # 定义需要保留的列
        self.activity_order_columns = [
            'id',
            'order_id', 
            'activity_id',
            'activity_name',
            'user_id',
            'order_status',
            'create_date',
        ]
        
        self.order_sub_columns = [
            'id',
            'order_id',
            'sub_order_id',
            'goods_id',
            'goods_name',
            'goods_num',
            'create_date'
        ]
        
    def load_data(self):
        """加载数据文件"""
        try:
            # 读取活动订单数据
            activity_order_path = self.input_dir / 'activity_order.csv'
            logger.info(f"正在读取活动订单数据: {activity_order_path}")
            
            self.activity_order = pd.read_csv(
                activity_order_path, 
                encoding='utf-8',
                low_memory=False  # 避免混合类型警告
            )
            logger.info(f"活动订单数据加载完成，形状: {self.activity_order.shape}")
            
            # 读取所有活动数据
            all_activity_path = self.input_dir / 'all_activity.csv'
            logger.info(f"正在读取所有活动数据: {all_activity_path}")
            
            self.all_activity = pd.read_csv(
                all_activity_path,
                encoding='utf-8',
                low_memory=False
            )
            logger.info(f"所有活动数据加载完成，形状: {self.all_activity.shape}")
            
            # 读取订单子表数据（如果存在）
            order_sub_path = self.input_dir / 'order_sub.csv'
            if order_sub_path.exists():
                logger.info(f"正在读取订单子表数据: {order_sub_path}")
                self.order_sub = pd.read_csv(
                    order_sub_path,
                    encoding='utf-8',
                    low_memory=False
                )
                logger.info(f"订单子表数据加载完成，形状: {self.order_sub.shape}")
            else:
                logger.warning("订单子表文件不存在，跳过")
                self.order_sub = None
                
        except Exception as e:
            logger.error(f"数据加载失败: {e}")
            raise
    
    def simplify_data(self):
        """简化数据，只保留需要的列"""
        try:
            # 简化活动订单数据
            logger.info("正在简化活动订单数据...")
            self.activity_order = self.activity_order[self.activity_order_columns]
            logger.info(f"简化后的活动订单数据形状: {self.activity_order.shape}")
            
            # 简化订单子表数据
            if self.order_sub is not None:
                logger.info("正在简化订单子表数据...")
                self.order_sub = self.order_sub[self.order_sub_columns]
                logger.info(f"简化后的订单子表数据形状: {self.order_sub.shape}")
                
        except Exception as e:
            logger.error(f"数据简化失败: {e}")
            raise
    
    def save_simplified_data(self):
        """保存简化后的数据到简化目录"""
        try:
            # 保存简化的活动订单数据
            simplified_activity_order_path = self.simplified_dir / 'activity_order_simplified.csv'
            self.activity_order.to_csv(simplified_activity_order_path, index=False, encoding='utf-8')
            logger.info(f"简化活动订单数据已保存: {simplified_activity_order_path}")
            
            # 保存简化的订单子表数据
            if self.order_sub is not None:
                simplified_order_sub_path = self.simplified_dir / 'order_sub_simplified.csv'
                self.order_sub.to_csv(simplified_order_sub_path, index=False, encoding='utf-8')
                logger.info(f"简化订单子表数据已保存: {simplified_order_sub_path}")
                
        except Exception as e:
            logger.error(f"保存简化数据失败: {e}")
            raise
    
    def generate_activity_statistics(self):
        """生成活动统计信息"""
        try:
            logger.info("正在生成活动统计信息...")
            
            # 按活动名称统计
            activity_name_counts = self.activity_order.groupby('activity_name').size()
            logger.info(f"活动名称统计完成，共 {len(activity_name_counts)} 个活动")
            
            # 按活动ID统计
            activity_id_counts = self.activity_order.groupby('activity_id').size()
            activity_id_counts_df = activity_id_counts.reset_index(name='count')
            
            # 检查活动ID是否在所有活动表中
            activity_id_counts_df['is_in_all_activity'] = activity_id_counts_df['activity_id'].isin(self.all_activity['id'])
            
            # 合并活动名称信息
            merged_df = pd.merge(
                activity_id_counts_df, 
                self.all_activity[['id', 'activity_name']], 
                left_on='activity_id', 
                right_on='id', 
                how='left'
            )
            
            # 选择需要的列
            final_df = merged_df[['activity_id', 'activity_name', 'count', 'is_in_all_activity']]
            
            # 保存统计信息到data目录
            stats_path = self.simplified_dir / 'activity_statistics.csv'
            final_df.to_csv(stats_path, index=False, encoding='utf-8')
            logger.info(f"活动统计信息已保存: {stats_path}")
            
            return final_df
            
        except Exception as e:
            logger.error(f"生成活动统计信息失败: {e}")
            raise
    
    def split_activity_orders(self):
        """分割每个活动的订单数据"""
        try:
            logger.info("开始分割活动订单数据...")
            
            # 创建活动名称映射
            activity_name_map = self.all_activity.set_index('id')['activity_name'].to_dict()
            
            # 统计信息
            total_activities = len(self.all_activity)
            processed_activities = 0
            successful_splits = 0
            failed_splits = 0
            
            # 遍历所有活动ID
            for activity_id in self.all_activity['id']:
                processed_activities += 1
                
                # 获取相同活动ID的订单
                activity_orders = self.activity_order[self.activity_order['activity_id'] == activity_id]
                
                # 如果没有订单，则跳过
                if activity_orders.empty:
                    activity_name = activity_name_map.get(activity_id, 'Unknown')
                    logger.info(f'[{processed_activities}/{total_activities}] 活动 {activity_name} 没有订单数据')
                    continue
                
                # 使用活动ID作为文件名
                file_name = f"{activity_id}.csv"
                file_path = self.output_dir / file_name
                
                # 将订单数据写入到CSV文件
                try:
                    activity_orders.to_csv(file_path, index=False, encoding='utf-8')
                    activity_name = activity_name_map.get(activity_id, 'Unknown')
                    logger.info(f'[{processed_activities}/{total_activities}] 成功保存活动 {activity_name} 的订单数据: {file_path}')
                    successful_splits += 1
                    
                except OSError as e:
                    logger.error(f"保存文件失败: {file_path}, 错误: {e}")
                    failed_splits += 1
            
            logger.info(f"分割完成！总活动数: {total_activities}, 成功分割: {successful_splits}, 失败: {failed_splits}")
            
        except Exception as e:
            logger.error(f"分割活动订单数据失败: {e}")
            raise
    
    def count_output_files(self):
        """统计输出文件数量"""
        try:
            csv_files = glob.glob(str(self.output_dir / '*.csv'))
            csv_file_count = len(csv_files)
            logger.info(f'输出目录中共有 {csv_file_count} 个CSV文件')
            return csv_file_count
            
        except Exception as e:
            logger.error(f"统计输出文件失败: {e}")
            raise
    
    def run(self):
        """运行完整的分割流程"""
        try:
            logger.info("=== 开始活动订单数据分割流程 ===")
            
            # 1. 加载数据
            self.load_data()
            
            # 2. 简化数据
            self.simplify_data()
            
            # 3. 保存简化数据
            self.save_simplified_data()
            
            # 4. 生成活动统计信息
            self.generate_activity_statistics()
            
            # 5. 分割活动订单数据
            self.split_activity_orders()
            
            # 6. 统计输出文件
            file_count = self.count_output_files()
            
            logger.info("=== 活动订单数据分割流程完成 ===")
            logger.info(f"输出目录: {self.output_dir}")
            logger.info(f"输出文件数量: {file_count}")
            
        except Exception as e:
            logger.error(f"流程执行失败: {e}")
            raise

def main():
    """主函数"""
    parser = argparse.ArgumentParser(description='活动订单数据分割工具')
    parser.add_argument('--input-dir', default='data/output20250706', help='输入数据目录')
    parser.add_argument('--output-dir', default='data/activity_order_independent', help='输出目录')
    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], help='日志级别')
    
    args = parser.parse_args()
    
    # 设置日志级别
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # 创建分割器并运行
    splitter = ActivityOrderSplitter(args.input_dir, args.output_dir)
    splitter.run()

if __name__ == "__main__":
    main() 